{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jGs8GYDF31yq",
        "kLxDC-iC31y1",
        "7WxryeAA31y9",
        "jsoQAi5131zF",
        "tOJVnU_431zI",
        "XwHtbrbw31zM",
        "kNg8Z3Ih31zR",
        "_asNSIvz31zm",
        "-PBnOjAG31zu",
        "V-jlCDPX31zy",
        "dgtKWnU131z1",
        "pThn-oZt31z8",
        "w6BDDqJY31z_",
        "u4b9xSv7310E",
        "J9sq8wk9310H",
        "DGGCo6j2310U",
        "rWQsg6zX310X"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gLN5oQI31xW",
        "colab_type": "text"
      },
      "source": [
        "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
        "\n",
        "We'll be using four apporaches here:\n",
        "\n",
        "* Continuous Bag of Words Model\n",
        "* Neural Network Model with Glove Word Embeddings\n",
        "* RNN Models using Word Embeddings\n",
        "* Character Level RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAP8yQYG31xZ",
        "colab_type": "code",
        "outputId": "02d699c6-da20-4857-b8ab-48e027bc193f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as tud\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import os, math\n",
        "import random\n",
        "import copy\n",
        "import string\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "\n",
        "from split_data import split_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTn2HfkD31xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the random seeds so the experiments can be replicated exactly\n",
        "random.seed(72689)\n",
        "np.random.seed(72689)\n",
        "torch.manual_seed(72689)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(72689)\n",
        "\n",
        "# Global class labels.\n",
        "POS_LABEL = 'up'\n",
        "NEG_LABEL = 'down'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuRD3aW531xo",
        "colab_type": "text"
      },
      "source": [
        "**Reading in all the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-EIkP331xp",
        "colab_type": "code",
        "outputId": "9c094904-be63-4717-922f-6f89c2df6ab8",
        "colab": {}
      },
      "source": [
        "all_data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
        "all_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "all_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Date</th>\n",
              "      <th>Content</th>\n",
              "      <th>OpenMove</th>\n",
              "      <th>CloseMove</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Top U.S. General Praises Iran-Backed Shiite Mi...</td>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>The top commander of the U.S.-led coalition ag...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Extremists Turn to a Leader to Protect Western...</td>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>As the founder of the Traditionalist Worker Pa...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How Julian Assange evolved from pariah to paragon</td>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>President-elect Donald Trump tweeted some pra...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>House panel recommends cutting funding for Pla...</td>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>A House panel formed by Republicans to invest...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Missouri Bill: Gun-Banning Businesses Liable f...</td>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>As Missouri lawmakers convene for the 2017 leg...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title        Date  \\\n",
              "0  Top U.S. General Praises Iran-Backed Shiite Mi...  2017-01-04   \n",
              "1  Extremists Turn to a Leader to Protect Western...  2017-01-04   \n",
              "2  How Julian Assange evolved from pariah to paragon  2017-01-04   \n",
              "3  House panel recommends cutting funding for Pla...  2017-01-04   \n",
              "4  Missouri Bill: Gun-Banning Businesses Liable f...  2017-01-04   \n",
              "\n",
              "                                             Content  OpenMove  CloseMove  \n",
              "0  The top commander of the U.S.-led coalition ag...       1.0        1.0  \n",
              "1  As the founder of the Traditionalist Worker Pa...       1.0        1.0  \n",
              "2   President-elect Donald Trump tweeted some pra...       1.0        1.0  \n",
              "3   A House panel formed by Republicans to invest...       1.0        1.0  \n",
              "4  As Missouri lawmakers convene for the 2017 leg...       1.0        1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytyTxtnn31xv",
        "colab_type": "text"
      },
      "source": [
        "*Using a Small Subset of Data fro Development*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyCOH5n-31xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sample = all_data.sample(10000, random_state=68)\n",
        "data_sample.reset_index(drop=True, inplace=True)\n",
        "data_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5FCUsEX31x3",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing the Data For Feeding Into The Model**\n",
        "\n",
        "Preprocessing Involves (in our case):\n",
        "* Turning All Words into lower/upper case, Normalization\n",
        "* removing punctuations, accent marks and other diacritics\n",
        "* removing stop words, sparse terms, and particular words\n",
        "* Lemmatize using NLTK (It's generally better than Stemming, but way slower)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7_J7MQk31x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing all Punctuation\n",
        "def remove_punctuation(text):\n",
        "    more_puncs = '—'+ '’'+ '“'+ '”'+ '…'\n",
        "    return text.translate(str.maketrans('', '', string.punctuation+more_puncs))\n",
        "\n",
        "# Removing all Stop Words\n",
        "def remove_stopwords(text, stop_words):\n",
        "    text = word_tokenize(text)\n",
        "    return  \" \".join([i for i in text if i not in stop_words])\n",
        "\n",
        "def lemmetize(text, lemmatizer, pos_tag_dict):\n",
        "    text = word_tokenize(text)\n",
        "    pos = nltk.pos_tag(text)\n",
        "    results = []\n",
        "    for pair in pos:\n",
        "        tag = pos_tag_dict.get(pair[1][0],wordnet.NOUN)\n",
        "        results.append(lemmatizer.lemmatize(pair[0], tag))\n",
        "        \n",
        "    return \" \".join(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJUNPhCU31x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_data = all_data[['Content', 'CloseMove']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIPlxOkP31yE",
        "colab_type": "text"
      },
      "source": [
        "The pre_process function below performs all the preprocessing we defined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iTqqjEB31yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(df):\n",
        "    # Normalization\n",
        "#     df['Title'] = df['Title'].str.lower()\n",
        "    df['Content'] = df['Content'].str.lower()\n",
        "\n",
        "    # Removing Punctuation\n",
        "#     df['Title'] = df['Title'].apply(remove_punctuation)\n",
        "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
        "    \n",
        "    STOP_WORDS = set(stopwords.words('english'))\n",
        "    # Remove Stopwords\n",
        "#     df['Title'] = df['Title'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
        "    df['Content'] = df['Content'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
        "\n",
        "    # Lemmetization\n",
        "    lemmer = WordNetLemmatizer()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV\n",
        "               }\n",
        "#     df['Title'] = df['Title'].apply(lemmetize, args=(lemmer, tag_dict))\n",
        "    df['Content'] = df['Content'].apply(lemmetize, args=(lemmer, tag_dict))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAX0xDx031yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwH-d1t431yQ",
        "colab_type": "text"
      },
      "source": [
        "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHxhXwvS31yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processing in Parallel\n",
        "n_threads = mp.cpu_count()-1\n",
        "data_pieces = np.array_split(sub_data, n_threads)\n",
        "startTime = time.time()\n",
        "pool = mp.Pool(n_threads)\n",
        "data_sample = pd.concat(pool.map(pre_process, data_pieces))\n",
        "pool.close()\n",
        "pool.join()\n",
        "\n",
        "totalTime = time.time() - startTime\n",
        "print(\"Time taken in Pre-Processing: {}m {}s\".format(totalTime // 60, totalTime%60))\n",
        "data_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKole2He31yY",
        "colab_type": "text"
      },
      "source": [
        "**We drop the rows which exceed default python's csv field max limit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntg5GGJ831yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_max_len = 131072\n",
        "to_drop = []\n",
        "for i in range(data_sample.shape[0]):\n",
        "    if len(data_sample.iloc[i,0]) >= csv_max_len-1:\n",
        "        to_drop.append(i)\n",
        "        \n",
        "data_sample.drop(to_drop, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQbsqfw31yd",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the Data and Storing it such that torch text can easily ingest it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jntSbla31ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 68\n",
        "split_data(df=data_sample,prefix='prod',seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPrgTV2M31yj",
        "colab_type": "text"
      },
      "source": [
        "## **Preparing Data**\n",
        "* Building the Vocabulary (Using Spacy) | **MAX_VOCAB_SIZE** = 70000\n",
        "* Splitting the data for Test and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vho0d1WR31ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGs8GYDF31yq",
        "colab_type": "text"
      },
      "source": [
        "### Reading in Data Using TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThN7NlfF31yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_prod_sel = {1:\"dev\", 0:\"prod\"}\n",
        "prefix = dev_prod_sel[0]\n",
        "train, val, test = data.TabularDataset.splits(\n",
        "        path='./ProcessedData/', train=prefix+'_train.csv',\n",
        "        validation=prefix+'_val.csv', test=prefix+'_test.csv', format='csv',\n",
        "        fields=[('Text', TEXT), ('Label', LABEL)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe-KgCAY31yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 70000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGNwH4A31yz",
        "colab_type": "text"
      },
      "source": [
        "## **Bag Of Words Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLxDC-iC31y1",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEzXgDw631y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class BOWDataLoader(tud.Dataset):\n",
        "    def __init__(self, data, vocab_size, text, field):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.TEXT = text\n",
        "        self.LABEL = field\n",
        "        self.TEXT.build_vocab(data, max_size = vocab_size)\n",
        "        self.LABEL.build_vocab(data)\n",
        "        self.data = data\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return len(self.data.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        itm = torch.zeros(self.vocab_size+2)\n",
        "        for word in self.data[idx].Text:\n",
        "            itm[self.TEXT.vocab.stoi[word]] += 1\n",
        "        \n",
        "        # To Differentiate Train and Test data\n",
        "        if len(self.data.fields) == 2:\n",
        "            label = self.data[idx].Label\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "train_dataset = BOWDataLoader(train, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
        "val_dataset = BOWDataLoader(val, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
        "test_dataset = BOWDataLoader(test, MAX_VOCAB_SIZE, TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIz0cnU431y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WxryeAA31y9",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words Model Training Module\n",
        "\n",
        "Here we define the training and evaluation functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgMv4ffz31y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BOWTrainingModule():\n",
        "    \n",
        "    def __init__(self, model, batch_size):\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        # Batch Size\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Cuda Availability\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "                \n",
        "        # Loss Function\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    def train_epoch(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains a logistic regression model across all examples in the dataset.\n",
        "        \"\"\"\n",
        "        self.dataloader = tud.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.model.train()\n",
        "        for i, (X,y) in enumerate(self.dataloader):\n",
        "            X = X.float()\n",
        "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).long()\n",
        "            if self.cuda:\n",
        "                X  = X.cuda()\n",
        "                y = y.cuda()\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            predictions = self.model.forward(X)\n",
        "            \n",
        "            loss = self.loss_fn(predictions, y)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            if (i) % 25 == 0:\n",
        "                print(\"Iteration : {:4d} | Loss : {:4.4f}\".format(i, loss.item()))\n",
        "            \n",
        "            self.optimizer.step()\n",
        "        \n",
        "    def train_model(self, train_data, val_data, num_epocs = 2):\n",
        "        \"\"\"\n",
        "        Trains the model and saves the best model according to the validation score\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        accuracy = [0.]\n",
        "        for epoch in range(num_epocs):\n",
        "            self.train_epoch(train_data)\n",
        "            val_accuracy = self.evaluate(val_data)\n",
        "            print(\"Validation Accuracy: {:4.4f}\".format(val_accuracy))\n",
        "            if val_accuracy > max(accuracy):\n",
        "                best_model = copy.deepcopy(self.model)        \n",
        "            accuracy.append(val_accuracy)\n",
        "        \n",
        "        return best_model\n",
        "                \n",
        "    def evaluate(self, data):\n",
        "        self.model.eval()\n",
        "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for i, (X,y) in enumerate(dataloader):\n",
        "            X = X.float()\n",
        "            if self.cuda:\n",
        "                X = X.cuda()\n",
        "            predictions = self.model.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
        "            correct += (predictions == np.asarray(y, dtype=np.float64)).sum()\n",
        "            total += predictions.shape[0]\n",
        "        \n",
        "        return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsoQAi5131zF",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kbjr2ug31zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BOWClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Constructing a Logistic Regression Model\n",
        "        \"\"\"\n",
        "        super(BOWClassifier, self).__init__()\n",
        "        \n",
        "        # Linear layer\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Passes the data through the network and return the output\n",
        "        \"\"\"\n",
        "        result = self.fc(text)\n",
        "        return (result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJVnU_431zI",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRqMJJoG31zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = MAX_VOCAB_SIZE + 2\n",
        "OUTPUT_DIM = 2\n",
        "BATCH_SIZE = 64\n",
        "model = BOWClassifier(INPUT_DIM, OUTPUT_DIM)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwHtbrbw31zM",
        "colab_type": "text"
      },
      "source": [
        "### Training the BOW Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NAuilY231zN",
        "colab_type": "code",
        "outputId": "11bcf55c-0960-43c2-c77c-e052260279e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bow_trainer = BOWTrainingModule(model, BATCH_SIZE)\n",
        "best_bow_model = bow_trainer.train_model(train_dataset, val_dataset, num_epocs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration :    0 | Loss : 0.6933\n",
            "Iteration :   25 | Loss : 0.6823\n",
            "Iteration :   50 | Loss : 0.7543\n",
            "Iteration :   75 | Loss : 0.7001\n",
            "Iteration :  100 | Loss : 0.7558\n",
            "Iteration :  125 | Loss : 0.8163\n",
            "Iteration :  150 | Loss : 0.7412\n",
            "Iteration :  175 | Loss : 0.6882\n",
            "Iteration :  200 | Loss : 0.7221\n",
            "Iteration :  225 | Loss : 0.7458\n",
            "Iteration :  250 | Loss : 0.7762\n",
            "Iteration :  275 | Loss : 0.6335\n",
            "Iteration :  300 | Loss : 0.6829\n",
            "Iteration :  325 | Loss : 0.7780\n",
            "Iteration :  350 | Loss : 0.6827\n",
            "Iteration :  375 | Loss : 0.6732\n",
            "Iteration :  400 | Loss : 0.9911\n",
            "Iteration :  425 | Loss : 0.8563\n",
            "Iteration :  450 | Loss : 0.7798\n",
            "Iteration :  475 | Loss : 0.7535\n",
            "Validation Accuracy: 0.5811\n",
            "Iteration :    0 | Loss : 0.4825\n",
            "Iteration :   25 | Loss : 0.5236\n",
            "Iteration :   50 | Loss : 0.4894\n",
            "Iteration :   75 | Loss : 0.5556\n",
            "Iteration :  100 | Loss : 0.5200\n",
            "Iteration :  125 | Loss : 0.4149\n",
            "Iteration :  150 | Loss : 0.5863\n",
            "Iteration :  175 | Loss : 0.6131\n",
            "Iteration :  200 | Loss : 0.5814\n",
            "Iteration :  225 | Loss : 0.5233\n",
            "Iteration :  250 | Loss : 0.5404\n",
            "Iteration :  275 | Loss : 0.4861\n",
            "Iteration :  300 | Loss : 0.6555\n",
            "Iteration :  325 | Loss : 0.5499\n",
            "Iteration :  350 | Loss : 0.5141\n",
            "Iteration :  375 | Loss : 0.5738\n",
            "Iteration :  400 | Loss : 0.5948\n",
            "Iteration :  425 | Loss : 0.5535\n",
            "Iteration :  450 | Loss : 0.6463\n",
            "Iteration :  475 | Loss : 0.5170\n",
            "Validation Accuracy: 0.5531\n",
            "Iteration :    0 | Loss : 0.3826\n",
            "Iteration :   25 | Loss : 0.4040\n",
            "Iteration :   50 | Loss : 0.3579\n",
            "Iteration :   75 | Loss : 0.4188\n",
            "Iteration :  100 | Loss : 0.4278\n",
            "Iteration :  125 | Loss : 0.4627\n",
            "Iteration :  150 | Loss : 0.4923\n",
            "Iteration :  175 | Loss : 0.4886\n",
            "Iteration :  200 | Loss : 0.4492\n",
            "Iteration :  225 | Loss : 0.4215\n",
            "Iteration :  250 | Loss : 0.4708\n",
            "Iteration :  275 | Loss : 0.4709\n",
            "Iteration :  300 | Loss : 0.4289\n",
            "Iteration :  325 | Loss : 0.4649\n",
            "Iteration :  350 | Loss : 0.4933\n",
            "Iteration :  375 | Loss : 0.5080\n",
            "Iteration :  400 | Loss : 0.5106\n",
            "Iteration :  425 | Loss : 0.5656\n",
            "Iteration :  450 | Loss : 0.4703\n",
            "Iteration :  475 | Loss : 0.4321\n",
            "Validation Accuracy: 0.5784\n",
            "Iteration :    0 | Loss : 0.3448\n",
            "Iteration :   25 | Loss : 0.5187\n",
            "Iteration :   50 | Loss : 0.4362\n",
            "Iteration :   75 | Loss : 0.3350\n",
            "Iteration :  100 | Loss : 0.3581\n",
            "Iteration :  125 | Loss : 0.2654\n",
            "Iteration :  150 | Loss : 0.3398\n",
            "Iteration :  175 | Loss : 0.4972\n",
            "Iteration :  200 | Loss : 0.4148\n",
            "Iteration :  225 | Loss : 0.3601\n",
            "Iteration :  250 | Loss : 0.3947\n",
            "Iteration :  275 | Loss : 0.3849\n",
            "Iteration :  300 | Loss : 0.3861\n",
            "Iteration :  325 | Loss : 0.4381\n",
            "Iteration :  350 | Loss : 0.4006\n",
            "Iteration :  375 | Loss : 0.3834\n",
            "Iteration :  400 | Loss : 0.5253\n",
            "Iteration :  425 | Loss : 0.4353\n",
            "Iteration :  450 | Loss : 0.4086\n",
            "Iteration :  475 | Loss : 0.4290\n",
            "Validation Accuracy: 0.5669\n",
            "Iteration :    0 | Loss : 0.2807\n",
            "Iteration :   25 | Loss : 0.2724\n",
            "Iteration :   50 | Loss : 0.3451\n",
            "Iteration :   75 | Loss : 0.3212\n",
            "Iteration :  100 | Loss : 0.3628\n",
            "Iteration :  125 | Loss : 0.3515\n",
            "Iteration :  150 | Loss : 0.3236\n",
            "Iteration :  175 | Loss : 0.3636\n",
            "Iteration :  200 | Loss : 0.3432\n",
            "Iteration :  225 | Loss : 0.3257\n",
            "Iteration :  250 | Loss : 0.3241\n",
            "Iteration :  275 | Loss : 0.2922\n",
            "Iteration :  300 | Loss : 0.3509\n",
            "Iteration :  325 | Loss : 0.3958\n",
            "Iteration :  350 | Loss : 0.4490\n",
            "Iteration :  375 | Loss : 0.3163\n",
            "Iteration :  400 | Loss : 0.4051\n",
            "Iteration :  425 | Loss : 0.3557\n",
            "Iteration :  450 | Loss : 0.3485\n",
            "Iteration :  475 | Loss : 0.4221\n",
            "Validation Accuracy: 0.5607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNg8Z3Ih31zR",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqNZiTrQ31zS",
        "colab_type": "code",
        "outputId": "73c624dc-f95d-4627-851a-afa12edba556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bow_accuracy = BOWTrainingModule(best_bow_model, BATCH_SIZE).evaluate(test_dataset)\n",
        "print(\"Bag Of Words Model Accuracy : {:4.4f}\".format(bow_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag Of Words Model Accuracy : 0.5826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgLI-sf731zW",
        "colab_type": "text"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUGp983x31zX",
        "colab_type": "text"
      },
      "source": [
        "## **Glove Embeddings**\n",
        "\n",
        "We use Glove Embeddings throughout the notebook. Below are a few functions that help us load and transform the Glove Encodings as we want.\n",
        "\n",
        "**Refrence:**\n",
        "\n",
        "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58xMHht31zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove(path_file):\n",
        "    \"\"\"\n",
        "    Loads the Glove Pre-Trained Embeddings\n",
        "    \n",
        "    Args:\n",
        "        path_file: Path to the official glove embedding text file\n",
        "    \n",
        "    Returns: Dictionary {Word: [Embedding]}\n",
        "    \n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    print(\"Loading Glove Model ...\")\n",
        "    glove = {}\n",
        "    with open(path_file) as f:\n",
        "        for line in f:\n",
        "            tmp = line.split()\n",
        "            glove[tmp[0]] = np.asarray(tmp[1:], dtype=np.float64)\n",
        "    print(\"Glove Model Loaded in {} s\".format(time.time()-start_time))\n",
        "    return glove\n",
        "\n",
        "def gloveWordIndex(glove):\n",
        "    \"\"\"\n",
        "    Generates word to index mappings\n",
        "    0 --> <unk>\n",
        "    1 --> <pad>\n",
        "    Args:\n",
        "        Loaded Glove Model as a dict\n",
        "        \n",
        "    Returns:\n",
        "        word to index map {word:idx} and index to word map{idx:word}\n",
        "    \n",
        "    \"\"\"\n",
        "    w_i = {k:v+2 for v,k in enumerate(glove.keys())}\n",
        "    w_i['<unk>'] = 0\n",
        "    w_i['<pad>'] = 1\n",
        "    i_w = {v+2:k for v,k in enumerate(glove.keys())}\n",
        "    i_w[0] = '<unk>'\n",
        "    i_w[1] = '<pad>'\n",
        "    return w_i, i_w\n",
        "\n",
        "def getWeightMatrix(glove):\n",
        "    embd_dim = glove['a'].shape[0]\n",
        "    num_embeddings = len(glove.keys())\n",
        "    w_m = np.zeros((num_embeddings+2, embd_dim))\n",
        "    w_m[0] = np.random.rand(embd_dim)\n",
        "    w_m[1] = np.zeros(embd_dim)\n",
        "    for i, word in enumerate(glove.keys()):\n",
        "        w_m[i+2] = glove[word]\n",
        "    \n",
        "    return w_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqi6VP3q31za",
        "colab_type": "code",
        "outputId": "0578199b-7d2b-425d-c931-02b86e2454f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "glove = load_glove(\"Embeddings/glove.6B.100d.txt\")\n",
        "word_to_idx, idx_to_word = gloveWordIndex(glove)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model ...\n",
            "Glove Model Loaded in 11.158129692077637 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YmlfhrN31zc",
        "colab_type": "code",
        "outputId": "beed2ede-e1eb-43ed-8752-8875ff6a10ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "weight_matrix = getWeightMatrix(glove)\n",
        "weight_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400002, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R49KUyCg31zf",
        "colab_type": "text"
      },
      "source": [
        "--------\n",
        "## **Data Loader**\n",
        "\n",
        "We set up the data loader to pad the sequqnces and return us sequences of length 1200. If longer then trim them to 1200 words.\n",
        "\n",
        "We can also use pad-packed-sequence functions from PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB_DcdLb31zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = len(glove.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJn-zX831zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class NeuralNetDataLoader(tud.Dataset):\n",
        "    def __init__(self, data, word_to_idx, idx_to_word, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_Word = idx_to_word\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return len(self.data.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        MAX_LEN = 1200\n",
        "        itm = []\n",
        "        l = 0\n",
        "        for word in self.data[idx].Text:\n",
        "            indx = self.word_to_idx.get(word,0)\n",
        "            itm.append(indx)\n",
        "            l += 1\n",
        "            if l == MAX_LEN:\n",
        "                break\n",
        "        \n",
        "        if len(itm) < MAX_LEN:\n",
        "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
        "        \n",
        "        itm = torch.tensor(itm).long()\n",
        "        # To Differentiate Train and Test data\n",
        "        if len(self.data.fields) == 2:\n",
        "            label = self.data[idx].Label\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "train_dataset = NeuralNetDataLoader(train, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
        "val_dataset = NeuralNetDataLoader(val, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
        "test_dataset = NeuralNetDataLoader(test, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_asNSIvz31zm",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the Data Iterators\n",
        "\n",
        "Using the data loader we set-up above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Q4oPNy31zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQt7Pwl31zp",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "## **Training Module**\n",
        "This module contains the evaluate and training functios.\n",
        "\n",
        "This module will help train us all the future models we make"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6n0fu1G31zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainingModule():\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        \n",
        "    def train_epoch(self, iterator):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        self.model.train()\n",
        "        for i, (X,y) in enumerate(iterator):\n",
        "            self.optimizer.zero_grad()\n",
        "            X = X.long()\n",
        "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
        "            if self.cuda:\n",
        "                X = X.cuda()\n",
        "                y = y.cuda()\n",
        "            preds = self.model.forward(X).squeeze(1)\n",
        "            \n",
        "            loss = self.loss_fn(preds, y)\n",
        "            \n",
        "            acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
        "            if i % 25 == 0:\n",
        "                print(\"Iteration: {:4d} | Loss : {:4.4f} | Accuracy : {:4.4f}\".format(i, loss.item(), acc))\n",
        "                                \n",
        "            loss.backward()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc\n",
        "            \n",
        "            self.optimizer.step()\n",
        "        \n",
        "        return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
        "    \n",
        "        \n",
        "    def train_model(self, train_iterator, dev_iterator, num_epocs = 5):\n",
        "\n",
        "        val_acc = [0.]\n",
        "        for epoch in range(num_epocs):\n",
        "            ep_loss, ep_accu = self.train_epoch(train_iterator)\n",
        "            dev_acc = self.evaluate(dev_iterator)\n",
        "            print(\"Dev. Loss : {} | Dev. Accuracy : {}\".format(dev_acc[0], dev_acc[1]))\n",
        "            if dev_acc[1] > max(val_acc):\n",
        "                best_model = copy.deepcopy(self.model)\n",
        "            val_acc.append(dev_acc[1])\n",
        "\n",
        "        return best_model\n",
        "        \n",
        "    \n",
        "    def evaluate(self, iterator):\n",
        "        epoch_loss  = 0\n",
        "        epoch_acc = 0\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (X,y) in enumerate(iterator):\n",
        "                X = X.long()\n",
        "                y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
        "                if self.cuda:\n",
        "                    X = X.cuda()\n",
        "                    y = y.cuda()\n",
        "                preds = self.model.forward(X).squeeze(1)\n",
        "\n",
        "                loss = self.loss_fn(preds, y)\n",
        "                \n",
        "                acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc\n",
        "        \n",
        "        return epoch_loss/len(iterator), epoch_acc/len(iterator)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-r7TsqT31zs",
        "colab_type": "text"
      },
      "source": [
        "## **Neural Network based Model with Word Embeddings**\n",
        "\n",
        "We use a Neural Network now with Word Embeddings, whoose :\n",
        "* Input : A sentence\n",
        "* Output: Label : {UP, DOWN}\n",
        "\n",
        "The basic structure of a model class is as above. Functions like classify, evaluate and train will be defined along with pretrained word-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfWjXGY831zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, pad_index, embedding_weights):\n",
        "        \n",
        "        super().__init__()\n",
        "        embd_dim = embedding_weights.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)\n",
        "        \n",
        "        hid_dim1 = 64\n",
        "        hid_dim2 = 32\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "        \n",
        "        self.hd1 = nn.Linear(embd_dim, hid_dim1)\n",
        "        self.hd2 = nn.Linear(hid_dim1, hid_dim2)\n",
        "        self.out = nn.Linear(hid_dim2, output_dim)\n",
        "        \n",
        "        self.activate = nn.ReLU()\n",
        "    \n",
        "    def forward(self,text):\n",
        "        \n",
        "#         print(\"Text: \", text.shape)\n",
        "        embds = self.embedding(text)\n",
        "#         print(\"Embds: \", embds.shape)\n",
        "        mean_embd = torch.mean(embds, 1)\n",
        "#         print(\"Embedding:\", mean_embd.shape)\n",
        "        output = self.activate(self.hd1(mean_embd.float()))\n",
        "#         print(\"Layer 1: \",output.shape)\n",
        "        output = self.drop_out(output)\n",
        "        output = self.activate(self.hd2(output))\n",
        "        output = self.drop_out(output)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBnOjAG31zu",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Neural Net Model \n",
        "with the appropriate dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myxZuo9W31zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = weight_matrix.shape[0]\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "model = NeuralNetClassifier(INPUT_DIM, OUTPUT_DIM, PAD_IDX, weight_matrix)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-jlCDPX31zy",
        "colab_type": "text"
      },
      "source": [
        "### Training the Neural Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7JBzb3831zz",
        "colab_type": "code",
        "outputId": "00aef9ea-9382-4ea1-ed8e-3aaf6a78faa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "neural_trainer = TrainingModule(model)\n",
        "best_nn_model = neural_trainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.6915 | Accuracy : 0.5000\n",
            "Iteration:   25 | Loss : 0.6579 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6676 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.7013 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6647 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6749 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6432 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6840 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6920 | Accuracy : 0.5469\n",
            "Iteration:  225 | Loss : 0.6488 | Accuracy : 0.6875\n",
            "Iteration:  250 | Loss : 0.7026 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6555 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6742 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.7014 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6424 | Accuracy : 0.7031\n",
            "Iteration:  375 | Loss : 0.6509 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.6937 | Accuracy : 0.5312\n",
            "Iteration:  425 | Loss : 0.6732 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6645 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6796 | Accuracy : 0.5938\n",
            "Dev. Loss : 0.6759968775691408 | Dev. Accuracy : 0.5913068181818182\n",
            "Iteration:    0 | Loss : 0.7311 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6284 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6455 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.7057 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6516 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6653 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6353 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6764 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6858 | Accuracy : 0.5469\n",
            "Iteration:  225 | Loss : 0.6417 | Accuracy : 0.6875\n",
            "Iteration:  250 | Loss : 0.7087 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6481 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6791 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6912 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6385 | Accuracy : 0.7031\n",
            "Iteration:  375 | Loss : 0.6458 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.6970 | Accuracy : 0.5312\n",
            "Iteration:  425 | Loss : 0.6722 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6712 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6729 | Accuracy : 0.5938\n",
            "Dev. Loss : 0.6751480767221162 | Dev. Accuracy : 0.5913068181818182\n",
            "Iteration:    0 | Loss : 0.7467 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6354 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6473 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.6962 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6389 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6618 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6258 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6664 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6900 | Accuracy : 0.5469\n",
            "Iteration:  225 | Loss : 0.6335 | Accuracy : 0.7031\n",
            "Iteration:  250 | Loss : 0.7217 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6457 | Accuracy : 0.6562\n",
            "Iteration:  300 | Loss : 0.6719 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6844 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6255 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.6604 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.6965 | Accuracy : 0.5156\n",
            "Iteration:  425 | Loss : 0.6696 | Accuracy : 0.5625\n",
            "Iteration:  450 | Loss : 0.6690 | Accuracy : 0.6250\n",
            "Iteration:  475 | Loss : 0.6694 | Accuracy : 0.5781\n",
            "Dev. Loss : 0.6730634223331105 | Dev. Accuracy : 0.5913068181818182\n",
            "Iteration:    0 | Loss : 0.7252 | Accuracy : 0.4375\n",
            "Iteration:   25 | Loss : 0.6320 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.6246 | Accuracy : 0.6406\n",
            "Iteration:   75 | Loss : 0.6813 | Accuracy : 0.5312\n",
            "Iteration:  100 | Loss : 0.6219 | Accuracy : 0.6875\n",
            "Iteration:  125 | Loss : 0.6363 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6039 | Accuracy : 0.7500\n",
            "Iteration:  175 | Loss : 0.6516 | Accuracy : 0.5781\n",
            "Iteration:  200 | Loss : 0.6525 | Accuracy : 0.6094\n",
            "Iteration:  225 | Loss : 0.6190 | Accuracy : 0.6250\n",
            "Iteration:  250 | Loss : 0.6964 | Accuracy : 0.6094\n",
            "Iteration:  275 | Loss : 0.5897 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6654 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6381 | Accuracy : 0.5938\n",
            "Iteration:  350 | Loss : 0.5877 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.6332 | Accuracy : 0.5938\n",
            "Iteration:  400 | Loss : 0.6772 | Accuracy : 0.5781\n",
            "Iteration:  425 | Loss : 0.6832 | Accuracy : 0.5938\n",
            "Iteration:  450 | Loss : 0.6488 | Accuracy : 0.6406\n",
            "Iteration:  475 | Loss : 0.6068 | Accuracy : 0.7344\n",
            "Dev. Loss : 0.6893812602216547 | Dev. Accuracy : 0.5887310606060606\n",
            "Iteration:    0 | Loss : 0.6917 | Accuracy : 0.5312\n",
            "Iteration:   25 | Loss : 0.5385 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.5954 | Accuracy : 0.6719\n",
            "Iteration:   75 | Loss : 0.6321 | Accuracy : 0.6250\n",
            "Iteration:  100 | Loss : 0.5724 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6073 | Accuracy : 0.6719\n",
            "Iteration:  150 | Loss : 0.4794 | Accuracy : 0.7656\n",
            "Iteration:  175 | Loss : 0.5555 | Accuracy : 0.6719\n",
            "Iteration:  200 | Loss : 0.4949 | Accuracy : 0.7812\n",
            "Iteration:  225 | Loss : 0.5863 | Accuracy : 0.6406\n",
            "Iteration:  250 | Loss : 0.5755 | Accuracy : 0.6875\n",
            "Iteration:  275 | Loss : 0.4987 | Accuracy : 0.7812\n",
            "Iteration:  300 | Loss : 0.6219 | Accuracy : 0.6406\n",
            "Iteration:  325 | Loss : 0.5446 | Accuracy : 0.6562\n",
            "Iteration:  350 | Loss : 0.5175 | Accuracy : 0.7500\n",
            "Iteration:  375 | Loss : 0.5671 | Accuracy : 0.7188\n",
            "Iteration:  400 | Loss : 0.5401 | Accuracy : 0.6719\n",
            "Iteration:  425 | Loss : 0.6625 | Accuracy : 0.6406\n",
            "Iteration:  450 | Loss : 0.5695 | Accuracy : 0.6719\n",
            "Iteration:  475 | Loss : 0.5310 | Accuracy : 0.7812\n",
            "Dev. Loss : 0.7690748810768128 | Dev. Accuracy : 0.5746401515151515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgtKWnU131z1",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zALlg2ua31z1",
        "colab_type": "code",
        "outputId": "c617ad4e-eb79-49af-84f0-41d7ec5edbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neural_accuracy = TrainingModule(best_nn_model).evaluate(test_iter)\n",
        "print(\"Neural Network Model Accuracy : {:4.4f}\".format(neural_accuracy[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network Model Accuracy : 0.6086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlhCydE31z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPtA2mGj31z5",
        "colab_type": "text"
      },
      "source": [
        "## **Recurrent Neural Network (GRU) with Glove Embeddings**\n",
        "\n",
        "We use GRU as a RNN model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFhYJy4G31z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordRNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, pad_index, embedding_weights, drop_out = 0):\n",
        "        \n",
        "        super().__init__()\n",
        "        embd_dim = embedding_weights.shape[1]\n",
        "        self.nhid = hidden_dim\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)   \n",
        "        self.rnn = nn.GRU(embd_dim, hidden_dim, dropout=drop_out)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, text):\n",
        "        \n",
        "        embds = self.embedding(text).float()\n",
        "        embds = embds.permute(1,0,2)\n",
        "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
        "        if torch.cuda.is_available():\n",
        "            hidden = hidden.cuda() \n",
        "        out, hid = self.rnn(embds, hidden)    \n",
        "        hid = self.dropout(hid)\n",
        "        out = self.output(hid.squeeze(0))\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pThn-oZt31z8",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the RNN (GRU) Model\n",
        "\n",
        "with correct parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhtSfK3A31z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = weight_matrix.shape[0]\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "HIDDEN_DIM = 64\n",
        "model = WordRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, PAD_IDX, weight_matrix)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6BDDqJY31z_",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diBWjzUh310C",
        "colab_type": "code",
        "outputId": "f0526573-266c-4757-fbc2-b4aa4d4ec053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "RNN_Trainer = TrainingModule(model)\n",
        "best_rnn_model = RNN_Trainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.7071 | Accuracy : 0.4062\n",
            "Iteration:   25 | Loss : 0.6321 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6533 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.6945 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6567 | Accuracy : 0.6406\n",
            "Iteration:  125 | Loss : 0.6712 | Accuracy : 0.6094\n",
            "Iteration:  150 | Loss : 0.6371 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6807 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6889 | Accuracy : 0.5469\n",
            "Iteration:  225 | Loss : 0.6445 | Accuracy : 0.6875\n",
            "Iteration:  250 | Loss : 0.7060 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6463 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6830 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6964 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6373 | Accuracy : 0.7031\n",
            "Iteration:  375 | Loss : 0.6511 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.7018 | Accuracy : 0.5312\n",
            "Iteration:  425 | Loss : 0.6720 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6647 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6748 | Accuracy : 0.5781\n",
            "Dev. Loss : 0.6774779041608174 | Dev. Accuracy : 0.5913068181818182\n",
            "Iteration:    0 | Loss : 0.7356 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6298 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6498 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.7026 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6525 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6631 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6397 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6821 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6819 | Accuracy : 0.5781\n",
            "Iteration:  225 | Loss : 0.6357 | Accuracy : 0.7031\n",
            "Iteration:  250 | Loss : 0.7075 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6538 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6733 | Accuracy : 0.6094\n",
            "Iteration:  325 | Loss : 0.6882 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6327 | Accuracy : 0.7031\n",
            "Iteration:  375 | Loss : 0.6446 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.7020 | Accuracy : 0.5312\n",
            "Iteration:  425 | Loss : 0.6678 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6684 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6714 | Accuracy : 0.5938\n",
            "Dev. Loss : 0.677398170485641 | Dev. Accuracy : 0.5903598484848485\n",
            "Iteration:    0 | Loss : 0.7344 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6317 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.6479 | Accuracy : 0.6562\n",
            "Iteration:   75 | Loss : 0.7088 | Accuracy : 0.5156\n",
            "Iteration:  100 | Loss : 0.6624 | Accuracy : 0.6250\n",
            "Iteration:  125 | Loss : 0.6590 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6421 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6759 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6796 | Accuracy : 0.5781\n",
            "Iteration:  225 | Loss : 0.6385 | Accuracy : 0.7031\n",
            "Iteration:  250 | Loss : 0.7070 | Accuracy : 0.5000\n",
            "Iteration:  275 | Loss : 0.6483 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6669 | Accuracy : 0.6094\n",
            "Iteration:  325 | Loss : 0.6946 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6196 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.6571 | Accuracy : 0.6562\n",
            "Iteration:  400 | Loss : 0.6848 | Accuracy : 0.5469\n",
            "Iteration:  425 | Loss : 0.6668 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6543 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6689 | Accuracy : 0.5781\n",
            "Dev. Loss : 0.6820707570422779 | Dev. Accuracy : 0.588844696969697\n",
            "Iteration:    0 | Loss : 0.7378 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6265 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6418 | Accuracy : 0.6719\n",
            "Iteration:   75 | Loss : 0.6881 | Accuracy : 0.5312\n",
            "Iteration:  100 | Loss : 0.6481 | Accuracy : 0.6562\n",
            "Iteration:  125 | Loss : 0.6570 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6359 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6772 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6617 | Accuracy : 0.5781\n",
            "Iteration:  225 | Loss : 0.6218 | Accuracy : 0.7031\n",
            "Iteration:  250 | Loss : 0.6929 | Accuracy : 0.5156\n",
            "Iteration:  275 | Loss : 0.6339 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6320 | Accuracy : 0.6406\n",
            "Iteration:  325 | Loss : 0.6887 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6303 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.6375 | Accuracy : 0.6719\n",
            "Iteration:  400 | Loss : 0.6750 | Accuracy : 0.5469\n",
            "Iteration:  425 | Loss : 0.6709 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6444 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6543 | Accuracy : 0.5938\n",
            "Dev. Loss : 0.6901177576093963 | Dev. Accuracy : 0.5891287878787879\n",
            "Iteration:    0 | Loss : 0.7226 | Accuracy : 0.4531\n",
            "Iteration:   25 | Loss : 0.6484 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6176 | Accuracy : 0.6875\n",
            "Iteration:   75 | Loss : 0.6829 | Accuracy : 0.5312\n",
            "Iteration:  100 | Loss : 0.6241 | Accuracy : 0.6719\n",
            "Iteration:  125 | Loss : 0.6496 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.6374 | Accuracy : 0.7031\n",
            "Iteration:  175 | Loss : 0.6800 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6578 | Accuracy : 0.5781\n",
            "Iteration:  225 | Loss : 0.6108 | Accuracy : 0.7031\n",
            "Iteration:  250 | Loss : 0.6955 | Accuracy : 0.5156\n",
            "Iteration:  275 | Loss : 0.6361 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6292 | Accuracy : 0.6406\n",
            "Iteration:  325 | Loss : 0.6792 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6142 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.6291 | Accuracy : 0.6719\n",
            "Iteration:  400 | Loss : 0.6794 | Accuracy : 0.5469\n",
            "Iteration:  425 | Loss : 0.6721 | Accuracy : 0.6094\n",
            "Iteration:  450 | Loss : 0.6467 | Accuracy : 0.6094\n",
            "Iteration:  475 | Loss : 0.6565 | Accuracy : 0.5938\n",
            "Dev. Loss : 0.6932295831766996 | Dev. Accuracy : 0.58875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4b9xSv7310E",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cGDF7e1310F",
        "colab_type": "code",
        "outputId": "5f41a67f-7d98-4219-eccf-0b7a6620b8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "rnn_accuracy = TrainingModule(best_rnn_model).evaluate(test_iter)\n",
        "print(\"RNN Model Accuracy : {:4.4f}\".format(rnn_accuracy[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RNN Model Accuracy : 0.6082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgxLLboL310H",
        "colab_type": "text"
      },
      "source": [
        "## **Chacracter Level RNN Model** _With Letter Embeddings_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9sq8wk9310H",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader\n",
        "\n",
        "Here we need to do something different. We don't want to Normalize the data, remove punctuation or any lemmetization. We want the model to learn how all the differene characters work together and relate to each other. So we will manually create our own mappings from index to letters and use them in the data loader.\n",
        "\n",
        "Also, we won'e be using TorchText here, we'll just be using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5roT0tV7310I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I33kA6y310K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data, val_data, test_data = split_data(df = data_sample[['Content', 'CloseMove']],prefix='char_dev', seed = 68, ret=1)\n",
        "train_data = pd.read_csv(\"ProcessedData/char_prod_train.csv\")\n",
        "test_data = pd.read_csv(\"ProcessedData/char_prod_test.csv\")\n",
        "val_data = pd.read_csv(\"ProcessedData/char_prod_val.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rq9ehRZ310O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Character to Index Mapping\n",
        "char_to_idx = {v:i+2 for i,v in enumerate(string.printable)}\n",
        "char_to_idx['<unk>'] = 0\n",
        "char_to_idx['<pad>'] = 1\n",
        "\n",
        "# Index to Character Mapping\n",
        "idx_to_char = {char_to_idx[i]:i for i in char_to_idx}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqEbY05C310Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CHAR_VOCAB = len(idx_to_char)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "class NeuralNetDataLoader(tud.Dataset):\n",
        "    def __init__(self, data, char_to_idx, idx_to_char, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.idx_to_char = idx_to_char\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        MAX_LEN = 5600\n",
        "        itm = []\n",
        "        l = 0\n",
        "        for char in self.data.iloc[idx,0].strip():\n",
        "            indx = self.char_to_idx.get(char,0)\n",
        "            itm.append(indx)\n",
        "            l += 1\n",
        "            if l == MAX_LEN:\n",
        "                break\n",
        "        \n",
        "        if len(itm) < MAX_LEN:\n",
        "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
        "        \n",
        "        itm = torch.tensor(itm).long()\n",
        "        # To Differentiate Train and Test data\n",
        "        if self.data.shape[1] == 2:\n",
        "            label = self.data.iloc[idx,1]\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "train_dataset = NeuralNetDataLoader(train_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
        "val_dataset = NeuralNetDataLoader(val_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
        "test_dataset = NeuralNetDataLoader(test_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdf2-qrW310S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGGCo6j2310U",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noLvEJD6310U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, embd_dim, pad_index):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.nhid = hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings = input_dim, embedding_dim=embd_dim, padding_idx=pad_index)   \n",
        "        self.rnn = nn.GRU(embd_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout()\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embds = self.embedding(text).float()\n",
        "        embds = embds.permute(1,0,2)\n",
        "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
        "        if torch.cuda.is_available():\n",
        "            hidden = hidden.cuda() \n",
        "        out, hid = self.rnn(embds, hidden)       \n",
        "        hid = self.dropout(hid)\n",
        "        out = self.output(hid.squeeze(0))\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWQsg6zX310X",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfkNMTZi310Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(char_to_idx)\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "HIDDEN_DIM = 64\n",
        "EMBD_DIM = 128\n",
        "model = CharRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, EMBD_DIM, PAD_IDX)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdXrKTGD310Z",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plZDj1hb310a",
        "colab_type": "code",
        "outputId": "b2ac8566-7c27-4a0c-dc06-d5d59f7631e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "CharRNNTrainer = TrainingModule(model)\n",
        "best_charnn_model = CharRNNTrainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.7169 | Accuracy : 0.3906\n",
            "Iteration:   25 | Loss : 0.6416 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6762 | Accuracy : 0.6250\n",
            "Iteration:   75 | Loss : 0.6954 | Accuracy : 0.5781\n",
            "Iteration:  100 | Loss : 0.6771 | Accuracy : 0.5781\n",
            "Iteration:  125 | Loss : 0.6970 | Accuracy : 0.5312\n",
            "Iteration:  150 | Loss : 0.7089 | Accuracy : 0.5156\n",
            "Iteration:  175 | Loss : 0.6956 | Accuracy : 0.5781\n",
            "Iteration:  200 | Loss : 0.6826 | Accuracy : 0.5781\n",
            "Iteration:  225 | Loss : 0.5975 | Accuracy : 0.7812\n",
            "Iteration:  250 | Loss : 0.6398 | Accuracy : 0.7031\n",
            "Iteration:  275 | Loss : 0.6376 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6729 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6761 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6463 | Accuracy : 0.6719\n",
            "Iteration:  375 | Loss : 0.5997 | Accuracy : 0.7969\n",
            "Iteration:  400 | Loss : 0.6903 | Accuracy : 0.5781\n",
            "Iteration:  425 | Loss : 0.6503 | Accuracy : 0.6719\n",
            "Iteration:  450 | Loss : 0.6456 | Accuracy : 0.6719\n",
            "Iteration:  475 | Loss : 0.6728 | Accuracy : 0.6250\n",
            "Dev. Loss : 0.6757864785916877 | Dev. Accuracy : 0.5946969696969697\n",
            "Iteration:    0 | Loss : 0.7061 | Accuracy : 0.5312\n",
            "Iteration:   25 | Loss : 0.6221 | Accuracy : 0.7188\n",
            "Iteration:   50 | Loss : 0.6740 | Accuracy : 0.6250\n",
            "Iteration:   75 | Loss : 0.6905 | Accuracy : 0.5469\n",
            "Iteration:  100 | Loss : 0.6705 | Accuracy : 0.5781\n",
            "Iteration:  125 | Loss : 0.6768 | Accuracy : 0.5625\n",
            "Iteration:  150 | Loss : 0.7029 | Accuracy : 0.5156\n",
            "Iteration:  175 | Loss : 0.6855 | Accuracy : 0.5781\n",
            "Iteration:  200 | Loss : 0.6735 | Accuracy : 0.6094\n",
            "Iteration:  225 | Loss : 0.6125 | Accuracy : 0.7656\n",
            "Iteration:  250 | Loss : 0.6387 | Accuracy : 0.7031\n",
            "Iteration:  275 | Loss : 0.6448 | Accuracy : 0.6875\n",
            "Iteration:  300 | Loss : 0.6762 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6796 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6374 | Accuracy : 0.7344\n",
            "Iteration:  375 | Loss : 0.5911 | Accuracy : 0.8125\n",
            "Iteration:  400 | Loss : 0.6851 | Accuracy : 0.5938\n",
            "Iteration:  425 | Loss : 0.6664 | Accuracy : 0.6719\n",
            "Iteration:  450 | Loss : 0.6536 | Accuracy : 0.6875\n",
            "Iteration:  475 | Loss : 0.6650 | Accuracy : 0.6250\n",
            "Dev. Loss : 0.6757489385026874 | Dev. Accuracy : 0.5949810606060606\n",
            "Iteration:    0 | Loss : 0.6984 | Accuracy : 0.5312\n",
            "Iteration:   25 | Loss : 0.6287 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.6775 | Accuracy : 0.6250\n",
            "Iteration:   75 | Loss : 0.6851 | Accuracy : 0.5469\n",
            "Iteration:  100 | Loss : 0.6770 | Accuracy : 0.5938\n",
            "Iteration:  125 | Loss : 0.6796 | Accuracy : 0.5625\n",
            "Iteration:  150 | Loss : 0.7102 | Accuracy : 0.5312\n",
            "Iteration:  175 | Loss : 0.6871 | Accuracy : 0.5938\n",
            "Iteration:  200 | Loss : 0.6620 | Accuracy : 0.6094\n",
            "Iteration:  225 | Loss : 0.6036 | Accuracy : 0.7656\n",
            "Iteration:  250 | Loss : 0.6352 | Accuracy : 0.7188\n",
            "Iteration:  275 | Loss : 0.6324 | Accuracy : 0.7031\n",
            "Iteration:  300 | Loss : 0.6776 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6875 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6344 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.5975 | Accuracy : 0.8281\n",
            "Iteration:  400 | Loss : 0.6804 | Accuracy : 0.6094\n",
            "Iteration:  425 | Loss : 0.6591 | Accuracy : 0.6250\n",
            "Iteration:  450 | Loss : 0.6565 | Accuracy : 0.6562\n",
            "Iteration:  475 | Loss : 0.6680 | Accuracy : 0.6250\n",
            "Dev. Loss : 0.6761622999653671 | Dev. Accuracy : 0.5946022727272727\n",
            "Iteration:    0 | Loss : 0.6978 | Accuracy : 0.5312\n",
            "Iteration:   25 | Loss : 0.6300 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.6704 | Accuracy : 0.6250\n",
            "Iteration:   75 | Loss : 0.6826 | Accuracy : 0.5469\n",
            "Iteration:  100 | Loss : 0.6683 | Accuracy : 0.6094\n",
            "Iteration:  125 | Loss : 0.6706 | Accuracy : 0.5781\n",
            "Iteration:  150 | Loss : 0.6948 | Accuracy : 0.5469\n",
            "Iteration:  175 | Loss : 0.6784 | Accuracy : 0.5781\n",
            "Iteration:  200 | Loss : 0.6578 | Accuracy : 0.6094\n",
            "Iteration:  225 | Loss : 0.6025 | Accuracy : 0.7656\n",
            "Iteration:  250 | Loss : 0.6447 | Accuracy : 0.6875\n",
            "Iteration:  275 | Loss : 0.6408 | Accuracy : 0.6875\n",
            "Iteration:  300 | Loss : 0.6698 | Accuracy : 0.6094\n",
            "Iteration:  325 | Loss : 0.6920 | Accuracy : 0.5469\n",
            "Iteration:  350 | Loss : 0.6354 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.5987 | Accuracy : 0.8125\n",
            "Iteration:  400 | Loss : 0.6863 | Accuracy : 0.5625\n",
            "Iteration:  425 | Loss : 0.6605 | Accuracy : 0.6562\n",
            "Iteration:  450 | Loss : 0.6567 | Accuracy : 0.6406\n",
            "Iteration:  475 | Loss : 0.6603 | Accuracy : 0.6250\n",
            "Dev. Loss : 0.6762485525824807 | Dev. Accuracy : 0.5933712121212121\n",
            "Iteration:    0 | Loss : 0.6980 | Accuracy : 0.5312\n",
            "Iteration:   25 | Loss : 0.6270 | Accuracy : 0.7031\n",
            "Iteration:   50 | Loss : 0.6648 | Accuracy : 0.6094\n",
            "Iteration:   75 | Loss : 0.6899 | Accuracy : 0.5469\n",
            "Iteration:  100 | Loss : 0.6671 | Accuracy : 0.5938\n",
            "Iteration:  125 | Loss : 0.6712 | Accuracy : 0.5781\n",
            "Iteration:  150 | Loss : 0.7014 | Accuracy : 0.5156\n",
            "Iteration:  175 | Loss : 0.6813 | Accuracy : 0.5781\n",
            "Iteration:  200 | Loss : 0.6568 | Accuracy : 0.6094\n",
            "Iteration:  225 | Loss : 0.5957 | Accuracy : 0.7656\n",
            "Iteration:  250 | Loss : 0.6369 | Accuracy : 0.6875\n",
            "Iteration:  275 | Loss : 0.6380 | Accuracy : 0.6719\n",
            "Iteration:  300 | Loss : 0.6749 | Accuracy : 0.5938\n",
            "Iteration:  325 | Loss : 0.6809 | Accuracy : 0.5625\n",
            "Iteration:  350 | Loss : 0.6184 | Accuracy : 0.7188\n",
            "Iteration:  375 | Loss : 0.5901 | Accuracy : 0.8281\n",
            "Iteration:  400 | Loss : 0.6768 | Accuracy : 0.5938\n",
            "Iteration:  425 | Loss : 0.6625 | Accuracy : 0.6562\n",
            "Iteration:  450 | Loss : 0.6638 | Accuracy : 0.5938\n",
            "Iteration:  475 | Loss : 0.6588 | Accuracy : 0.6250\n",
            "Dev. Loss : 0.6770506782965227 | Dev. Accuracy : 0.5927083333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQhOpd_Z310b",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdB2q6yr310c",
        "colab_type": "code",
        "outputId": "2b7de85a-8a46-48e6-e818-88a0d0f3c8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "char_rnn_accuracy = TrainingModule(best_charnn_model).evaluate(test_iter)\n",
        "print(\"Character Level Character Accuracy: {:4.4f}\".format(char_rnn_accuracy[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Character Level Character Accuracy: 0.6029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtZL_ExPTA49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}