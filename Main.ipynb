{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
    "\n",
    "We'll be using two apporaches here:\n",
    "\n",
    "* Continuous Bag of Words Model\n",
    "* RNN Models using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/antimony/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antimony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/antimony/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/antimony/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(72689)\n",
    "np.random.seed(72689)\n",
    "torch.manual_seed(72689)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(72689)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'up'\n",
    "NEG_LABEL = 'down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in all the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top U.S. General Praises Iran-Backed Shiite Mi...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>The top commander of the U.S.-led coalition ag...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremists Turn to a Leader to Protect Western...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As the founder of the Traditionalist Worker Pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Julian Assange evolved from pariah to paragon</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>President-elect Donald Trump tweeted some pra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House panel recommends cutting funding for Pla...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>A House panel formed by Republicans to invest...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missouri Bill: Gun-Banning Businesses Liable f...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As Missouri lawmakers convene for the 2017 leg...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0  Top U.S. General Praises Iran-Backed Shiite Mi...  2017-01-04   \n",
       "1  Extremists Turn to a Leader to Protect Western...  2017-01-04   \n",
       "2  How Julian Assange evolved from pariah to paragon  2017-01-04   \n",
       "3  House panel recommends cutting funding for Pla...  2017-01-04   \n",
       "4  Missouri Bill: Gun-Banning Businesses Liable f...  2017-01-04   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  The top commander of the U.S.-led coalition ag...       1.0        1.0  \n",
       "1  As the founder of the Traditionalist Worker Pa...       1.0        1.0  \n",
       "2   President-elect Donald Trump tweeted some pra...       1.0        1.0  \n",
       "3   A House panel formed by Republicans to invest...       1.0        1.0  \n",
       "4  As Missouri lawmakers convene for the 2017 leg...       1.0        1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
    "all_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using a Small Subset of Data fro Development*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another government shutdown over Obamacare? On...</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>\\nThis is the web version of VoxCare, a daily ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tourists Helped Fatten Him Up; Now Thai Monkey...</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>[Whether he likes it or not, a morbidly obese ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indian Premier, in Israel Visit, Seeks to Brea...</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>JERUSALEM — Prime Minister Benjamin Netanyahu ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kiribati Ends Aerial Search for Missing Ferry ...</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>WELLINGTON, New Zealand — The aerial search fo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The American Model</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>Amidst       a string of pat introductory refl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0  Another government shutdown over Obamacare? On...  2017-04-24   \n",
       "1  Tourists Helped Fatten Him Up; Now Thai Monkey...  2017-05-19   \n",
       "2  Indian Premier, in Israel Visit, Seeks to Brea...  2017-07-05   \n",
       "3  Kiribati Ends Aerial Search for Missing Ferry ...  2018-03-21   \n",
       "4                                 The American Model  2017-07-28   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  \\nThis is the web version of VoxCare, a daily ...       1.0        1.0  \n",
       "1  [Whether he likes it or not, a morbidly obese ...       1.0        1.0  \n",
       "2  JERUSALEM — Prime Minister Benjamin Netanyahu ...       0.0        0.0  \n",
       "3  WELLINGTON, New Zealand — The aerial search fo...       0.0        0.0  \n",
       "4  Amidst       a string of pat introductory refl...       0.0        0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = all_data.sample(10000, random_state=68)\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data For Feeding Into The Model\n",
    "\n",
    "Preprocessing Involves (in our case):\n",
    "* Turning All Words into lower/upper case, Normalization\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* Lemmatize using NLTK (It's generally better than Stemming, but way slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Punctuation\n",
    "def remove_punctuation(text):\n",
    "    more_puncs = '—'+ '’'+ '“'+ '”'+ '…'\n",
    "    return text.translate(str.maketrans('', '', string.punctuation+more_puncs))\n",
    "\n",
    "# Removing all Stop Words\n",
    "def remove_stopwords(text, stop_words):\n",
    "    text = word_tokenize(text)\n",
    "    return  \" \".join([i for i in text if i not in stop_words])\n",
    "\n",
    "def lemmetize(text, lemmatizer, pos_tag_dict):\n",
    "    text = word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    results = []\n",
    "    for pair in pos:\n",
    "        tag = pos_tag_dict.get(pair[1][0],wordnet.NOUN)\n",
    "        results.append(lemmatizer.lemmatize(pair[0], tag))\n",
    "        \n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre_process function below performs all the preprocessing we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    # Normalization\n",
    "    df['Title'] = df['Title'].str.lower()\n",
    "    df['Content'] = df['Content'].str.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "    df['Title'] = df['Title'].apply(remove_punctuation)\n",
    "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
    "    \n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "    # Remove Stopwords\n",
    "    df['Title'] = df['Title'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "    df['Content'] = df['Content'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "\n",
    "    # Lemmetization\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV\n",
    "               }\n",
    "    df['Title'] = df['Title'].apply(lemmetize, args=(lemmer, tag_dict))\n",
    "    df['Content'] = df['Content'].apply(lemmetize, args=(lemmer, tag_dict))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3af2a0431950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtotalTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time taken in Pre-Processing: {}m {}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalTime\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalTime\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "# Processing in Parallel\n",
    "n_threads = mp.cpu_count()-1\n",
    "data_pieces = np.array_split(data_sample, n_threads)\n",
    "startTime = time.time()\n",
    "pool = mp.Pool(n_threads)\n",
    "data_sample = pd.concat(pool.map(pre_process, data_pieces))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "totalTime = time.time() - startTime\n",
    "print(\"Time taken in Pre-Processing: {}m {}s\".format(totalTime // 60, totalTime%60))\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for the CBOW Model\n",
    "\n",
    "* Building the Vocabulary (Using Spacy) | **MAX_VOCAB_SIZE** = 25000\n",
    "* Splitting the data for Test and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data and Storing it such that torch text can easily ingest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d6650b7a458a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CloseMove'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_sample' is not defined"
     ]
    }
   ],
   "source": [
    "split_data(df=data_sample[['Content', 'CloseMove']],prefix='dev',seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Data Using TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./ProcessedData/', train='dev_train.csv',\n",
    "        validation='dev_val.csv', test='dev_test.csv', format='csv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 70000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class BOWDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, vocab_size, text, field):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.TEXT = text\n",
    "        self.LABEL = field\n",
    "        self.TEXT.build_vocab(data, max_size = vocab_size)\n",
    "        self.LABEL.build_vocab(data)\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of Examples\n",
    "        '''\n",
    "        return len(self.data.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of text and label at the given index.\n",
    "        If label is not present None is returned.\n",
    "        \"\"\"\n",
    "        itm = torch.zeros(self.vocab_size)\n",
    "        for word in self.data[idx].Text:\n",
    "            itm[self.TEXT.vocab.stoi[word]] += 1\n",
    "        \n",
    "        # To Differentiate Train and Test data\n",
    "        if len(self.data.fields) == 2:\n",
    "            label = self.data[idx].Label\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = BOWDataLoader(train, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "val_dataset = BOWDataLoader(val, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "test_dataset = BOWDataLoader(test, MAX_VOCAB_SIZE, TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model\n",
    "\n",
    "Here we define:\n",
    "* The INPUT, OUTPUT dimrensions.\n",
    "* The Loss Function and\n",
    "* The optimizer\n",
    "\n",
    "for the model.\n",
    "\n",
    "\n",
    "Also, the training, classification and evaluation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "class BOWClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, batch_size):\n",
    "        \"\"\"\n",
    "        Constructing a Logistic Regression Model\n",
    "        \n",
    "        Loss Function: Cross Entropy Loss\n",
    "        Optimizer: Adam\n",
    "        \"\"\"\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # Cuda Availability\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        \n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # Activation \n",
    "#         self.activate = nn.Sigmoid()\n",
    "        \n",
    "        # Loss Function\n",
    "        self. loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Passes the data through the network and return the output\n",
    "        \"\"\"\n",
    "        result = self.fc(text)\n",
    "        return (result)\n",
    "    \n",
    "    def train_epoch(self, dataset):\n",
    "        \"\"\"\n",
    "        Trains a logistic regression model across all examples in the dataset.\n",
    "        \"\"\"\n",
    "        self.dataloader = tud.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.train()\n",
    "        for i, (X,y) in enumerate(self.dataloader):\n",
    "            X = X.float()\n",
    "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).long()\n",
    "            if self.cuda:\n",
    "                X  = X.cuda()\n",
    "                y = y.cuda()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            loss = self.loss_fn(predictions, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"Iteration : {:4d} | Loss : {:4.4f}\".format(i+1, loss.item()))\n",
    "            \n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def train_model(self, train_data, val_data, num_epocs = 2):\n",
    "        \"\"\"\n",
    "        Trains the model and saves the best model according to the validation score\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        accuracy = [0.]\n",
    "        for epoch in range(num_epocs):\n",
    "            self.train_epoch(train_data)\n",
    "            val_accuracy = self.evaluate(val_data)\n",
    "            print(\"Validation Accuracy: {:4.4f}\".format(val_accuracy))\n",
    "            if val_accuracy > max(accuracy):\n",
    "                best_model = copy.deepcopy(self)        \n",
    "            accuracy.append(val_accuracy)\n",
    "            \n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Returns the results of the dataset passed.\n",
    "        \"\"\"\n",
    "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
    "        results = np.asarray([])\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "            predictions = self.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
    "            results = np.append(results, predictions)\n",
    "        labels = [\"UP\" if i == 1 else \"DOWN\" for i in results]\n",
    "        return labels\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "            predictions = self.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
    "            correct += (predictions == np.asarray(y, dtype=np.float64)).sum()\n",
    "            total += predictions.shape[0]\n",
    "        \n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BOWClassifier(MAX_VOCAB_SIZE, 2, 8)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# model.train_model(train_dataset, val_dataset, num_epocs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network based Model with Word Embeddings\n",
    "\n",
    "We use a Neural Network now with Word Embeddings, whoose :\n",
    "* Input : A sentence\n",
    "* Output: Label : {UP, DOWN}\n",
    "\n",
    "The basic structure of a model class is as above. Functions like classify, evaluate and train will be defined along with pretrained word-embeddings.\n",
    "\n",
    "The **work embeddings** we use are **Glove**.\n",
    "\n",
    "**Refrence:**\n",
    "\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path_file):\n",
    "    \"\"\"\n",
    "    Loads the Glove Pre-Trained Embeddings\n",
    "    \n",
    "    Args:\n",
    "        path_file: Path to the official glove embedding text file\n",
    "    \n",
    "    Returns: Dictionary {Word: [Embedding]}\n",
    "    \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Loading Glove Model ...\")\n",
    "    glove = {}\n",
    "    with open(path_file) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split()\n",
    "            glove[tmp[0]] = np.asarray(tmp[1:], dtype=np.float64)\n",
    "    print(\"Glove Model Loaded in {} s\".format(time.time()-start_time))\n",
    "    return glove\n",
    "\n",
    "def gloveWordIndex(glove):\n",
    "    \"\"\"\n",
    "    Generates word to index mappings\n",
    "    0 --> <unk>\n",
    "    1 --> <pad>\n",
    "    Args:\n",
    "        Loaded Glove Model as a dict\n",
    "        \n",
    "    Returns:\n",
    "        word to index map {word:idx} and index to word map{idx:word}\n",
    "    \n",
    "    \"\"\"\n",
    "    w_i = {k:v+2 for v,k in enumerate(glove.keys())}\n",
    "    w_i['<unk>'] = 0\n",
    "    w_i['<pad>'] = 1\n",
    "    i_w = {v+2:k for v,k in enumerate(glove.keys())}\n",
    "    i_w[0] = '<unk>'\n",
    "    i_w[1] = '<pad>'\n",
    "    return w_i, i_w\n",
    "\n",
    "def getWeightMatrix(glove):\n",
    "    embd_dim = glove['a'].shape[0]\n",
    "    num_embeddings = len(glove.keys())\n",
    "    w_m = np.zeros((num_embeddings+2, embd_dim))\n",
    "    w_m[0] = np.random.rand(embd_dim)\n",
    "    w_m[1] = np.zeros(embd_dim)\n",
    "    for i, word in enumerate(glove.keys()):\n",
    "        w_m[i+2] = glove[word]\n",
    "    \n",
    "    return w_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model ...\n",
      "Glove Model Loaded in 10.682385921478271 s\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove(\"Embeddings/glove.6B.100d.txt\")\n",
    "word_to_idx, idx_to_word = gloveWordIndex(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix = getWeightMatrix(glove)\n",
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = len(glove.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NeuralNetDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, word_to_idx, idx_to_word, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_Word = idx_to_word\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of Examples\n",
    "        '''\n",
    "        return len(self.data.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of text and label at the given index.\n",
    "        If label is not present None is returned.\n",
    "        \"\"\"\n",
    "        MAX_LEN = 1000\n",
    "        itm = []\n",
    "        l = 0\n",
    "        for word in self.data[idx].Text:\n",
    "            indx = self.word_to_idx.get(word,0)\n",
    "            itm.append(indx)\n",
    "            l += 1\n",
    "            if l == MAX_LEN:\n",
    "                break\n",
    "        \n",
    "        if len(itm) < MAX_LEN:\n",
    "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
    "        \n",
    "        itm = torch.tensor(itm).long()\n",
    "        # To Differentiate Train and Test data\n",
    "        if len(self.data.fields) == 2:\n",
    "            label = self.data[idx].Label\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = NeuralNetDataLoader(train, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
    "val_dataset = NeuralNetDataLoader(val, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
    "test_dataset = NeuralNetDataLoader(test, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model with Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, pad_index, embedding_weights):\n",
    "        \n",
    "        super().__init__()\n",
    "        embd_dim = embedding_weights.shape[1]\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)\n",
    "        \n",
    "        hid_dim1 = 64\n",
    "        hid_dim2 = 32\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.hd1 = nn.Linear(embd_dim, hid_dim1)\n",
    "        self.hd2 = nn.Linear(hid_dim1, hid_dim2)\n",
    "        self.out = nn.Linear(hid_dim2, output_dim)\n",
    "        \n",
    "        self.activate = nn.ReLU()\n",
    "        self.loss_fn = None\n",
    "    \n",
    "    def forward(self,text):\n",
    "        \n",
    "#         print(\"Text: \", text.shape)\n",
    "        embds = self.embedding(text)\n",
    "#         print(\"Embds: \", embds.shape)\n",
    "        mean_embd = torch.mean(embds, 1)\n",
    "#         print(\"Embedding:\", mean_embd.shape)\n",
    "        output = self.activate(self.hd1(mean_embd.float()))\n",
    "#         print(\"Layer 1: \",output.shape)\n",
    "        output = self.drop_out(output)\n",
    "        output = self.activate(self.hd2(output))\n",
    "        output = self.drop_out(output)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Module\n",
    "This module contains the evaluate and training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        for i, (X,y) in enumerate(iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = X.long()\n",
    "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            preds = self.model.forward(X).squeeze(1)\n",
    "            \n",
    "            loss = self.loss_fn(preds, y)\n",
    "            \n",
    "            acc = (preds==y).sum()/y.shape[0]\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iteration: \")\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "    \n",
    "        \n",
    "    def train_model(self, train_iterator, dev_iterator, num_epocs = 5):\n",
    "\n",
    "        dev_acc =[0]\n",
    "        for epoch in range(num_epocs):\n",
    "            ep_loss, ep_accu = self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(\"Dev. Accuracy : {} | Dev. Format : {}\".format(dev_acc[0], dev_acc[1]))\n",
    "            if dev_acc[1] > max(dev_acc):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_acc.append(dev_acc[1])\n",
    "\n",
    "        return best_model.model\n",
    "        \n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss  = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                \n",
    "                predictions = self.model(batch.text).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.label)\n",
    "                acc = binary_accuracy(predictions, batch.label)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc = acc.item()\n",
    "                \n",
    "        \n",
    "        return epoch_loss/len(iterator), epoch_acc/len(iterator)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model \n",
    "with the appropriate dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetClassifier(\n",
      "  (embedding): Embedding(400002, 100, padding_idx=1)\n",
      "  (drop_out): Dropout(p=0.5)\n",
      "  (hd1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (hd2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (activate): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = weight_matrix.shape[0]\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = 1\n",
    "BATCH_SIZE = 64\n",
    "model = NeuralNetClassifier(INPUT_DIM, OUTPUT_DIM, PAD_IDX, weight_matrix)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  torch.Size([64, 1000])\n",
      "Embds:  torch.Size([64, 1000, 100])\n",
      "Embedding: torch.Size([64, 100])\n",
      "Layer 1:  torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "for (X,y) in train_iter:\n",
    "    t = model.forward(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
