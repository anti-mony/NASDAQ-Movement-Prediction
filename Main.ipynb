{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
    "\n",
    "We'll be using two apporaches here:\n",
    "\n",
    "* Continuous Bag of Words Model\n",
    "* RNN Models using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/antimony/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antimony/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(72689)\n",
    "np.random.seed(72689)\n",
    "torch.manual_seed(72689)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(72689)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'up'\n",
    "NEG_LABEL = 'down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in all the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top U.S. General Praises Iran-Backed Shiite Mi...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>The top commander of the U.S.-led coalition ag...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremists Turn to a Leader to Protect Western...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As the founder of the Traditionalist Worker Pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Julian Assange evolved from pariah to paragon</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>President-elect Donald Trump tweeted some pra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House panel recommends cutting funding for Pla...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>A House panel formed by Republicans to invest...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missouri Bill: Gun-Banning Businesses Liable f...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As Missouri lawmakers convene for the 2017 leg...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0  Top U.S. General Praises Iran-Backed Shiite Mi...  2017-01-04   \n",
       "1  Extremists Turn to a Leader to Protect Western...  2017-01-04   \n",
       "2  How Julian Assange evolved from pariah to paragon  2017-01-04   \n",
       "3  House panel recommends cutting funding for Pla...  2017-01-04   \n",
       "4  Missouri Bill: Gun-Banning Businesses Liable f...  2017-01-04   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  The top commander of the U.S.-led coalition ag...       1.0        1.0  \n",
       "1  As the founder of the Traditionalist Worker Pa...       1.0        1.0  \n",
       "2   President-elect Donald Trump tweeted some pra...       1.0        1.0  \n",
       "3   A House panel formed by Republicans to invest...       1.0        1.0  \n",
       "4  As Missouri lawmakers convene for the 2017 leg...       1.0        1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data For Feeding Into The Model\n",
    "\n",
    "Preprocessing Involves (in our case):\n",
    "* Turning All Words into lower/upper case, Normalization\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* Stemming using a Porter Stemmer from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Removing all Stop Words\n",
    "def remove_stopwords(text):\n",
    "    text = word_tokenize(text)\n",
    "    return  \" \".join([i for i in text if i not in STOP_WORDS])\n",
    "\n",
    "def stem(text, stemmer):\n",
    "    text = word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(i) for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre_process function below performs all the preprocessing we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    # Normalization\n",
    "    data['Title'] = data['Title'].str.lower()\n",
    "    data['Content'] = data['Content'].str.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "    df['Title'] = df['Title'].apply(remove_punctuation)\n",
    "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
    "    \n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "    # Remove Stopwords\n",
    "    df['Title'] = df['Title'].apply(remove_stopwords)\n",
    "    df['Content'] = df['Content'].apply(remove_stopwords)\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    df['Title'] = df['Title'].apply(stem, (stemmer,))\n",
    "    df['Content'] = df['Content'].apply(stem, (stemmer,))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/antimony/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/share/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/corpus/util.py\", line 86, in __load\n    root = nltk.data.find('{}/{}'.format(self.subdir, zip_name))\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/data.py\", line 701, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/antimony/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/share/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-44-7e5f2e054e97>\", line 10, in pre_process\n    STOP_WORDS = set(stopwords.words('english'))\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/corpus/util.py\", line 123, in __getattr__\n    self.__load()\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/corpus/util.py\", line 88, in __load\n    raise e\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/corpus/util.py\", line 83, in __load\n    root = nltk.data.find('{}/{}'.format(self.subdir, self.__name))\n  File \"/home/antimony/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/nltk/data.py\", line 701, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/antimony/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/share/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-14265d4d990e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_pieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MachineLearning/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MachineLearning/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/antimony/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/share/nltk_data'\n    - '/home/antimony/miniconda3/envs/MachineLearning/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Processing in Parallel\n",
    "n_threads = mp.cpu_count()-1\n",
    "data_pieces = np.array_split(data, n_threads)\n",
    "\n",
    "pool = mp.Pool(n_threads)\n",
    "data = pd.concat(pool.map(pre_process, data_pieces))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
