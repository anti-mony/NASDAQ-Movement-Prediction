{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
    "\n",
    "We'll be using two apporaches here:\n",
    "\n",
    "* Continuous Bag of Words Model\n",
    "* RNN Models using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/antimony/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antimony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "\n",
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(72689)\n",
    "np.random.seed(72689)\n",
    "torch.manual_seed(72689)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(72689)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'up'\n",
    "NEG_LABEL = 'down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in all the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top U.S. General Praises Iran-Backed Shiite Mi...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>The top commander of the U.S.-led coalition ag...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extremists Turn to a Leader to Protect Western...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As the founder of the Traditionalist Worker Pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Julian Assange evolved from pariah to paragon</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>President-elect Donald Trump tweeted some pra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House panel recommends cutting funding for Pla...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>A House panel formed by Republicans to invest...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missouri Bill: Gun-Banning Businesses Liable f...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>As Missouri lawmakers convene for the 2017 leg...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0  Top U.S. General Praises Iran-Backed Shiite Mi...  2017-01-04   \n",
       "1  Extremists Turn to a Leader to Protect Western...  2017-01-04   \n",
       "2  How Julian Assange evolved from pariah to paragon  2017-01-04   \n",
       "3  House panel recommends cutting funding for Pla...  2017-01-04   \n",
       "4  Missouri Bill: Gun-Banning Businesses Liable f...  2017-01-04   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  The top commander of the U.S.-led coalition ag...       1.0        1.0  \n",
       "1  As the founder of the Traditionalist Worker Pa...       1.0        1.0  \n",
       "2   President-elect Donald Trump tweeted some pra...       1.0        1.0  \n",
       "3   A House panel formed by Republicans to invest...       1.0        1.0  \n",
       "4  As Missouri lawmakers convene for the 2017 leg...       1.0        1.0  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
    "all_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using a Small Subset of Data fro Development*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another government shutdown over Obamacare? On...</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>\\nThis is the web version of VoxCare, a daily ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tourists Helped Fatten Him Up; Now Thai Monkey...</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>[Whether he likes it or not, a morbidly obese ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indian Premier, in Israel Visit, Seeks to Brea...</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>JERUSALEM — Prime Minister Benjamin Netanyahu ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kiribati Ends Aerial Search for Missing Ferry ...</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>WELLINGTON, New Zealand — The aerial search fo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The American Model</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>Amidst       a string of pat introductory refl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0  Another government shutdown over Obamacare? On...  2017-04-24   \n",
       "1  Tourists Helped Fatten Him Up; Now Thai Monkey...  2017-05-19   \n",
       "2  Indian Premier, in Israel Visit, Seeks to Brea...  2017-07-05   \n",
       "3  Kiribati Ends Aerial Search for Missing Ferry ...  2018-03-21   \n",
       "4                                 The American Model  2017-07-28   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  \\nThis is the web version of VoxCare, a daily ...       1.0        1.0  \n",
       "1  [Whether he likes it or not, a morbidly obese ...       1.0        1.0  \n",
       "2  JERUSALEM — Prime Minister Benjamin Netanyahu ...       0.0        0.0  \n",
       "3  WELLINGTON, New Zealand — The aerial search fo...       0.0        0.0  \n",
       "4  Amidst       a string of pat introductory refl...       0.0        0.0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = all_data.sample(10000, random_state=68)\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data For Feeding Into The Model\n",
    "\n",
    "Preprocessing Involves (in our case):\n",
    "* Turning All Words into lower/upper case, Normalization\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* Stemming using a Porter Stemmer from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Punctuation\n",
    "def remove_punctuation(text):\n",
    "    more_puncs = '—'+ '’'+ '“'+ '”'+ '…'\n",
    "    return text.translate(str.maketrans('', '', string.punctuation+more_puncs))\n",
    "\n",
    "# Removing all Stop Words\n",
    "def remove_stopwords(text, stop_words):\n",
    "    text = word_tokenize(text)\n",
    "    return  \" \".join([i for i in text if i not in stop_words])\n",
    "\n",
    "def stem(text, stemmer):\n",
    "    text = word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(i) for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre_process function below performs all the preprocessing we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    # Normalization\n",
    "    df['Title'] = df['Title'].str.lower()\n",
    "    df['Content'] = df['Content'].str.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "    df['Title'] = df['Title'].apply(remove_punctuation)\n",
    "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
    "    \n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "    # Remove Stopwords\n",
    "    df['Title'] = df['Title'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "    df['Content'] = df['Content'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    df['Title'] = df['Title'].apply(stem, args=(stemmer,))\n",
    "    df['Content'] = df['Content'].apply(stem, args=(stemmer,))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>OpenMove</th>\n",
       "      <th>CloseMove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anoth govern shutdown obamacar trump want</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>web version voxcar daili newslett vox latest t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tourist help fatten thai monkey diet</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>whether like morbidli obes wild monkey thailan...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indian premier israel visit seek break barrier...</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>jerusalem prime minist benjamin netanyahu long...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kiribati end aerial search miss ferri passeng ...</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>wellington new zealand aerial search ferri kir...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american model</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>amidst string pat introductori reflect recent ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date  \\\n",
       "0          anoth govern shutdown obamacar trump want  2017-04-24   \n",
       "1               tourist help fatten thai monkey diet  2017-05-19   \n",
       "2  indian premier israel visit seek break barrier...  2017-07-05   \n",
       "3  kiribati end aerial search miss ferri passeng ...  2018-03-21   \n",
       "4                                     american model  2017-07-28   \n",
       "\n",
       "                                             Content  OpenMove  CloseMove  \n",
       "0  web version voxcar daili newslett vox latest t...       1.0        1.0  \n",
       "1  whether like morbidli obes wild monkey thailan...       1.0        1.0  \n",
       "2  jerusalem prime minist benjamin netanyahu long...       0.0        0.0  \n",
       "3  wellington new zealand aerial search ferri kir...       0.0        0.0  \n",
       "4  amidst string pat introductori reflect recent ...       0.0        0.0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing in Parallel\n",
    "n_threads = mp.cpu_count()-1\n",
    "data_pieces = np.array_split(data_sample, n_threads)\n",
    "\n",
    "pool = mp.Pool(n_threads)\n",
    "data_sample = pd.concat(pool.map(pre_process, data_pieces))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for the CBOW Model\n",
    "\n",
    "* Building the Vocabulary (Using Spacy) | **MAX_VOCAB_SIZE** = 25000\n",
    "* Splitting the data for Test and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data and Storing it such that torch text can easily ingest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(df=data_sample[['Content', 'CloseMove']],prefix='dev',seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Data Using TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./ProcessedData/', train='dev_train.csv',\n",
    "        validation='dev_val.csv', test='dev_test.csv', format='csv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class BOWDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, vocab_size, text, field):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.TEXT = text\n",
    "        self.LABEL = field\n",
    "        self.TEXT.build_vocab(data, max_size = MAX_VOCAB_SIZE)\n",
    "        self.LABEL.build_vocab(data)\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        itm = torch.zeros(self.vocab_size)\n",
    "        for word in self.data[idx].Text:\n",
    "            itm[self.TEXT.vocab.stoi[word]] += 1\n",
    "        \n",
    "        # To Differentiate Train and Test data\n",
    "        if len(self.data.fields) == 2:\n",
    "            label = self.data[idx].Label\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = BOWDataLoader(train, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "val_dataset = BOWDataLoader(val, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "test_dataset = BOWDataLoader(test, MAX_VOCAB_SIZE, TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model\n",
    "\n",
    "Here we define:\n",
    "* The INPUT, OUTPUT dimrensions.\n",
    "* The Loss Function and\n",
    "* The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, batch_size):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # Cuda Availability\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        \n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # Activation \n",
    "#         self.activate = nn.Sigmoid()\n",
    "        \n",
    "        # Loss Function\n",
    "        self. loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def forward(self, text):\n",
    "        result = self.fc(text)\n",
    "        return (result)\n",
    "    \n",
    "    def train_epoch(self, dataset):\n",
    "        self.dataloader = tud.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.train()\n",
    "        for i, (X,y) in enumerate(self.dataloader):\n",
    "            X = X.float()\n",
    "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).long()\n",
    "            if self.cuda:\n",
    "                X  = X.cuda()\n",
    "                y = y.cuda()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            loss = self.loss_fn(predictions, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"Iteration : {:4d} | Loss : {:4.4f}\".format(i+1, loss.item()))\n",
    "            \n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def train_model(self, train_data, val_data, num_epocs = 2):\n",
    "        accuracy = 0\n",
    "        for epoch in range(num_epocs):\n",
    "            self.train_epoch(train_data)\n",
    "            val_accuracy = self.evaluate(val_data)\n",
    "            print(\"Validation Accuracy: {:4.4f}\".format(val_accuracy))\n",
    "            if val_accuracy > accuracy:\n",
    "                accuracy = val_accuracy\n",
    "                best_model = copy.deepcopy(self)        \n",
    "    \n",
    "    def classify(self, data):\n",
    "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
    "        results = np.asarray([])\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "            predictions = self.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
    "            results = np.append(results, predictions)\n",
    "        labels = [\"UP\" if i == 1 else \"DOWN\" for i in results]\n",
    "        return labels\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "            predictions = self.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
    "            correct += (predictions == np.asarray(y, dtype=np.float64)).sum()\n",
    "            total += predictions.shape[0]\n",
    "        \n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  100 | Loss : 0.9569\n",
      "Iteration :  200 | Loss : 0.6613\n",
      "Iteration :  300 | Loss : 0.9165\n",
      "Iteration :  400 | Loss : 0.7737\n",
      "Iteration :  500 | Loss : 0.6956\n",
      "Iteration :  600 | Loss : 0.9078\n",
      "Validation Accuracy: 0.5686\n",
      "Iteration :  100 | Loss : 0.1815\n",
      "Iteration :  200 | Loss : 0.4284\n",
      "Iteration :  300 | Loss : 0.4284\n",
      "Iteration :  400 | Loss : 0.3501\n",
      "Iteration :  500 | Loss : 0.4605\n",
      "Iteration :  600 | Loss : 0.3707\n",
      "Validation Accuracy: 0.5348\n",
      "Iteration :  100 | Loss : 0.1545\n",
      "Iteration :  200 | Loss : 0.2308\n",
      "Iteration :  300 | Loss : 0.0866\n",
      "Iteration :  400 | Loss : 0.2568\n",
      "Iteration :  500 | Loss : 0.1381\n",
      "Iteration :  600 | Loss : 0.3024\n",
      "Validation Accuracy: 0.5338\n",
      "Iteration :  100 | Loss : 0.2689\n",
      "Iteration :  200 | Loss : 0.1747\n",
      "Iteration :  300 | Loss : 0.0883\n",
      "Iteration :  400 | Loss : 0.3071\n",
      "Iteration :  500 | Loss : 0.2298\n",
      "Iteration :  600 | Loss : 0.1342\n",
      "Validation Accuracy: 0.5514\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-99fa136da570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBOWClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-cde5cb0fc4c7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_data, val_data, num_epocs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Accuracy: {:4.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-cde5cb0fc4c7>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MachineLearning/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-14fc4a1d4726>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mitm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mitm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# To Differentiate Train and Test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BOWClassifier(MAX_VOCAB_SIZE, 2, 8)\n",
    "model.train_model(train_dataset, val_dataset, num_epocs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
