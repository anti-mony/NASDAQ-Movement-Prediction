{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
    "\n",
    "We'll be using four apporaches here:\n",
    "\n",
    "* Continuous Bag of Words Model\n",
    "* Neural Network Model with Glove Word Embeddings\n",
    "* RNN Models using Word Embeddings\n",
    "* Character Level RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(72689)\n",
    "np.random.seed(72689)\n",
    "torch.manual_seed(72689)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(72689)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'up'\n",
    "NEG_LABEL = 'down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in all the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
    "all_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using a Small Subset of Data fro Development*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = all_data.sample(10000, random_state=68)\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the Data For Feeding Into The Model**\n",
    "\n",
    "Preprocessing Involves (in our case):\n",
    "* Turning All Words into lower/upper case, Normalization\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* Lemmatize using NLTK (It's generally better than Stemming, but way slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Punctuation\n",
    "def remove_punctuation(text):\n",
    "    more_puncs = '—'+ '’'+ '“'+ '”'+ '…'\n",
    "    return text.translate(str.maketrans('', '', string.punctuation+more_puncs))\n",
    "\n",
    "# Removing all Stop Words\n",
    "def remove_stopwords(text, stop_words):\n",
    "    text = word_tokenize(text)\n",
    "    return  \" \".join([i for i in text if i not in stop_words])\n",
    "\n",
    "def lemmetize(text, lemmatizer, pos_tag_dict):\n",
    "    text = word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    results = []\n",
    "    for pair in pos:\n",
    "        tag = pos_tag_dict.get(pair[1][0],wordnet.NOUN)\n",
    "        results.append(lemmatizer.lemmatize(pair[0], tag))\n",
    "        \n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = all_data[['Content', 'CloseMove']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre_process function below performs all the preprocessing we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    # Normalization\n",
    "#     df['Title'] = df['Title'].str.lower()\n",
    "    df['Content'] = df['Content'].str.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "#     df['Title'] = df['Title'].apply(remove_punctuation)\n",
    "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
    "    \n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "    # Remove Stopwords\n",
    "#     df['Title'] = df['Title'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "    df['Content'] = df['Content'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
    "\n",
    "    # Lemmetization\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV\n",
    "               }\n",
    "#     df['Title'] = df['Title'].apply(lemmetize, args=(lemmer, tag_dict))\n",
    "    df['Content'] = df['Content'].apply(lemmetize, args=(lemmer, tag_dict))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing in Parallel\n",
    "n_threads = mp.cpu_count()-1\n",
    "data_pieces = np.array_split(sub_data, n_threads)\n",
    "startTime = time.time()\n",
    "pool = mp.Pool(n_threads)\n",
    "data_sample = pd.concat(pool.map(pre_process, data_pieces))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "totalTime = time.time() - startTime\n",
    "print(\"Time taken in Pre-Processing: {}m {}s\".format(totalTime // 60, totalTime%60))\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We drop the rows which exceed default python's csv field max limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_max_len = 131072\n",
    "to_drop = []\n",
    "for i in range(data_sample.shape[0]):\n",
    "    if len(data_sample.iloc[i,0]) >= csv_max_len-1:\n",
    "        to_drop.append(i)\n",
    "        \n",
    "data_sample.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the Data and Storing it such that torch text can easily ingest it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 68\n",
    "split_data(df=data_sample,prefix='prod',seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparing Data**\n",
    "* Building the Vocabulary (Using Spacy) | **MAX_VOCAB_SIZE** = 70000\n",
    "* Splitting the data for Test and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data Using TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./ProcessedData/', train='prod_train.csv',\n",
    "        validation='prod_val.csv', test='prod_test.csv', format='csv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 70000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class BOWDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, vocab_size, text, field):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.TEXT = text\n",
    "        self.LABEL = field\n",
    "        self.TEXT.build_vocab(data, max_size = vocab_size)\n",
    "        self.LABEL.build_vocab(data)\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of Examples\n",
    "        '''\n",
    "        return len(self.data.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of text and label at the given index.\n",
    "        If label is not present None is returned.\n",
    "        \"\"\"\n",
    "        itm = torch.zeros(self.vocab_size)\n",
    "        for word in self.data[idx].Text:\n",
    "            itm[self.TEXT.vocab.stoi[word]] += 1\n",
    "        \n",
    "        # To Differentiate Train and Test data\n",
    "        if len(self.data.fields) == 2:\n",
    "            label = self.data[idx].Label\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = BOWDataLoader(train, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "val_dataset = BOWDataLoader(val, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
    "test_dataset = BOWDataLoader(test, MAX_VOCAB_SIZE, TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words Model Training Module**\n",
    "\n",
    "Here we define the training and evaluation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWTrainingModule():\n",
    "    \n",
    "    def __init__(self, model, batch_size):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        # Batch Size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Cuda Availability\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "                \n",
    "        # Loss Function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    def train_epoch(self, dataset):\n",
    "        \"\"\"\n",
    "        Trains a logistic regression model across all examples in the dataset.\n",
    "        \"\"\"\n",
    "        self.dataloader = tud.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.model.train()\n",
    "        for i, (X,y) in enumerate(self.dataloader):\n",
    "            X = X.float()\n",
    "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).long()\n",
    "            if self.cuda:\n",
    "                X  = X.cuda()\n",
    "                y = y.cuda()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            predictions = self.model.forward(X)\n",
    "            \n",
    "            loss = self.loss_fn(predictions, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"Iteration : {:4d} | Loss : {:4.4f}\".format(i+1, loss.item()))\n",
    "            \n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def train_model(self, train_data, val_data, num_epocs = 2):\n",
    "        \"\"\"\n",
    "        Trains the model and saves the best model according to the validation score\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        accuracy = [0.]\n",
    "        for epoch in range(num_epocs):\n",
    "            self.train_epoch(train_data)\n",
    "            val_accuracy = self.evaluate(val_data)\n",
    "            print(\"Validation Accuracy: {:4.4f}\".format(val_accuracy))\n",
    "            if val_accuracy > max(accuracy):\n",
    "                best_model = copy.deepcopy(self)        \n",
    "            accuracy.append(val_accuracy)\n",
    "        \n",
    "        return best_model\n",
    "                \n",
    "    def evaluate(self, data):\n",
    "        self.model.eval()\n",
    "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            X = X.float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "            predictions = self.model.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
    "            correct += (predictions == np.asarray(y, dtype=np.float64)).sum()\n",
    "            total += predictions.shape[0]\n",
    "        \n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag Of Words Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Constructing a Logistic Regression Model\n",
    "        \"\"\"\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        \n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Passes the data through the network and return the output\n",
    "        \"\"\"\n",
    "        result = self.fc(text)\n",
    "        return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = MAX_VOCAB_SIZE\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 64\n",
    "model = BOWClassifier(MAX_VOCAB_SIZE, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_trainer = BOWTrainingModule(model, BATCH_SIZE)\n",
    "bow_trainer.train_model(train_dataset, val_dataset, num_epocs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_accuracy = bow_trainer.evaluate(test_dataset)\n",
    "print(\"Bag Of Words Model Accuracy: {:4.4f}\".format(bow_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Glove Embeddings**\n",
    "\n",
    "We use Glove Embeddings throughout the notebook. Below are a few functions that help us load and transform the Glove Encodings as we want.\n",
    "\n",
    "**Refrence:**\n",
    "\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path_file):\n",
    "    \"\"\"\n",
    "    Loads the Glove Pre-Trained Embeddings\n",
    "    \n",
    "    Args:\n",
    "        path_file: Path to the official glove embedding text file\n",
    "    \n",
    "    Returns: Dictionary {Word: [Embedding]}\n",
    "    \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Loading Glove Model ...\")\n",
    "    glove = {}\n",
    "    with open(path_file) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split()\n",
    "            glove[tmp[0]] = np.asarray(tmp[1:], dtype=np.float64)\n",
    "    print(\"Glove Model Loaded in {} s\".format(time.time()-start_time))\n",
    "    return glove\n",
    "\n",
    "def gloveWordIndex(glove):\n",
    "    \"\"\"\n",
    "    Generates word to index mappings\n",
    "    0 --> <unk>\n",
    "    1 --> <pad>\n",
    "    Args:\n",
    "        Loaded Glove Model as a dict\n",
    "        \n",
    "    Returns:\n",
    "        word to index map {word:idx} and index to word map{idx:word}\n",
    "    \n",
    "    \"\"\"\n",
    "    w_i = {k:v+2 for v,k in enumerate(glove.keys())}\n",
    "    w_i['<unk>'] = 0\n",
    "    w_i['<pad>'] = 1\n",
    "    i_w = {v+2:k for v,k in enumerate(glove.keys())}\n",
    "    i_w[0] = '<unk>'\n",
    "    i_w[1] = '<pad>'\n",
    "    return w_i, i_w\n",
    "\n",
    "def getWeightMatrix(glove):\n",
    "    embd_dim = glove['a'].shape[0]\n",
    "    num_embeddings = len(glove.keys())\n",
    "    w_m = np.zeros((num_embeddings+2, embd_dim))\n",
    "    w_m[0] = np.random.rand(embd_dim)\n",
    "    w_m[1] = np.zeros(embd_dim)\n",
    "    for i, word in enumerate(glove.keys()):\n",
    "        w_m[i+2] = glove[word]\n",
    "    \n",
    "    return w_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = load_glove(\"Embeddings/glove.6B.100d.txt\")\n",
    "word_to_idx, idx_to_word = gloveWordIndex(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = getWeightMatrix(glove)\n",
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## **Data Loader**\n",
    "\n",
    "We set up the data loader to pad the sequqnces and return us sequences of length 1200. If longer then trim them to 1200 words.\n",
    "\n",
    "We can also use pad-packed-sequence functions from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = len(glove.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NeuralNetDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, word_to_idx, idx_to_word, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_Word = idx_to_word\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of Examples\n",
    "        '''\n",
    "        return len(self.data.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of text and label at the given index.\n",
    "        If label is not present None is returned.\n",
    "        \"\"\"\n",
    "        MAX_LEN = 1200\n",
    "        itm = []\n",
    "        l = 0\n",
    "        for word in self.data[idx].Text:\n",
    "            indx = self.word_to_idx.get(word,0)\n",
    "            itm.append(indx)\n",
    "            l += 1\n",
    "            if l == MAX_LEN:\n",
    "                break\n",
    "        \n",
    "        if len(itm) < MAX_LEN:\n",
    "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
    "        \n",
    "        itm = torch.tensor(itm).long()\n",
    "        # To Differentiate Train and Test data\n",
    "        if len(self.data.fields) == 2:\n",
    "            label = self.data[idx].Label\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = NeuralNetDataLoader(train, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
    "val_dataset = NeuralNetDataLoader(val, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
    "test_dataset = NeuralNetDataLoader(test, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Data Iterators\n",
    "\n",
    "Using the data loader we set-up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## **Training Module**\n",
    "This module contains the evaluate and training functios.\n",
    "\n",
    "This module will help train us all the future models we make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        for i, (X,y) in enumerate(iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = X.long()\n",
    "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
    "            if self.cuda:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            preds = self.model.forward(X).squeeze(1)\n",
    "            \n",
    "            loss = self.loss_fn(preds, y)\n",
    "            \n",
    "            acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
    "            if i % 20 == 0:\n",
    "                print(\"Iteration: {} | Loss : {:4.4f} | Accuracy : {:4.4f}\".format(i, loss.item(), acc))\n",
    "                                \n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            \n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "    \n",
    "        \n",
    "    def train_model(self, train_iterator, dev_iterator, num_epocs = 5):\n",
    "\n",
    "        val_acc = [0.]\n",
    "        for epoch in range(num_epocs):\n",
    "            ep_loss, ep_accu = self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(\"Dev. Loss : {} | Dev. Accuracy : {}\".format(dev_acc[0], dev_acc[1]))\n",
    "            if dev_acc[1] > max(val_acc):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            val_acc.append(dev_acc[1])\n",
    "\n",
    "        return best_model.model\n",
    "        \n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss  = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (X,y) in enumerate(iterator):\n",
    "                X = X.long()\n",
    "                y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
    "                if self.cuda:\n",
    "                    X = X.cuda()\n",
    "                    y = y.cuda()\n",
    "                preds = self.model.forward(X).squeeze(1)\n",
    "\n",
    "                loss = self.loss_fn(preds, y)\n",
    "                \n",
    "                acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "        \n",
    "        return epoch_loss/len(iterator), epoch_acc/len(iterator)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network based Model with Word Embeddings**\n",
    "\n",
    "We use a Neural Network now with Word Embeddings, whoose :\n",
    "* Input : A sentence\n",
    "* Output: Label : {UP, DOWN}\n",
    "\n",
    "The basic structure of a model class is as above. Functions like classify, evaluate and train will be defined along with pretrained word-embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, pad_index, embedding_weights):\n",
    "        \n",
    "        super().__init__()\n",
    "        embd_dim = embedding_weights.shape[1]\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)\n",
    "        \n",
    "        hid_dim1 = 64\n",
    "        hid_dim2 = 32\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.hd1 = nn.Linear(embd_dim, hid_dim1)\n",
    "        self.hd2 = nn.Linear(hid_dim1, hid_dim2)\n",
    "        self.out = nn.Linear(hid_dim2, output_dim)\n",
    "        \n",
    "        self.activate = nn.ReLU()\n",
    "    \n",
    "    def forward(self,text):\n",
    "        \n",
    "#         print(\"Text: \", text.shape)\n",
    "        embds = self.embedding(text)\n",
    "#         print(\"Embds: \", embds.shape)\n",
    "        mean_embd = torch.mean(embds, 1)\n",
    "#         print(\"Embedding:\", mean_embd.shape)\n",
    "        output = self.activate(self.hd1(mean_embd.float()))\n",
    "#         print(\"Layer 1: \",output.shape)\n",
    "        output = self.drop_out(output)\n",
    "        output = self.activate(self.hd2(output))\n",
    "        output = self.drop_out(output)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Neural Net Model \n",
    "with the appropriate dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = weight_matrix.shape[0]\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = 1\n",
    "model = NeuralNetClassifier(INPUT_DIM, OUTPUT_DIM, PAD_IDX, weight_matrix)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_trainer = TrainingModule(model)\n",
    "neural_trainer.train_model(train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_accuracy = neural_trainer(test_iter)\n",
    "print(\"Neural Network Model Accuracy : {:4.4f}\".format(neural_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recurrent Neural Network (GRU) with Glove Embeddings**\n",
    "\n",
    "We use GRU as a RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, pad_index, embedding_weights, drop_out = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "        embd_dim = embedding_weights.shape[1]\n",
    "        self.nhid = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)   \n",
    "        self.rnn = nn.GRU(embd_dim, hidden_dim, dropout=drop_out)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        embds = self.embedding(text).float()\n",
    "        embds = embds.permute(1,0,2)\n",
    "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = hidden.cuda() \n",
    "        out, hid = self.rnn(embds, hidden)       \n",
    "        out = self.dropout(out)\n",
    "        out = self.output(hid.squeeze(0))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the RNN (GRU) Model\n",
    "\n",
    "with correct parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = weight_matrix.shape[0]\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = 1\n",
    "HIDDEN_DIM = 64\n",
    "model = WordRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, PAD_IDX, weight_matrix)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_Trainer = TrainingModule(model)\n",
    "RNN_Trainer.train_model(train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_accuracy = RNN_Trainer.evaluate(test_iter)\n",
    "print(\"RNN Model Accuracy : {:4.4f}\".format(nn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chacracter Level RNN Model** _With Letter Embeddings_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Here we need to do something different. We don't want to Normalize the data, remove punctuation or any lemmetization. We want the model to learn how all the differene characters work together and relate to each other. So we will manually create our own mappings from index to letters and use them in the data loader.\n",
    "\n",
    "Also, we won'e be using TorchText here, we'll just be using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = split_data(df = data_sample[['Content', 'CloseMove']],prefix='char_dev', seed = 68, ret=1)\n",
    "train_data = pd.read_csv(\"ProcessedData/char_dev_train.csv\")\n",
    "test_data = pd.read_csv(\"ProcessedData/char_dev_test.csv\")\n",
    "val_data = pd.read_csv(\"ProcessedData/char_dev_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character to Index Mapping\n",
    "char_to_idx = {v:i+2 for i,v in enumerate(string.printable)}\n",
    "char_to_idx['<unk>'] = 0\n",
    "char_to_idx['<pad>'] = 1\n",
    "\n",
    "# Index to Character Mapping\n",
    "idx_to_char = {char_to_idx[i]:i for i in char_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHAR_VOCAB = len(idx_to_char)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NeuralNetDataLoader(tud.Dataset):\n",
    "    def __init__(self, data, char_to_idx, idx_to_char, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of Examples\n",
    "        '''\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of text and label at the given index.\n",
    "        If label is not present None is returned.\n",
    "        \"\"\"\n",
    "        MAX_LEN = 5600\n",
    "        itm = []\n",
    "        l = 0\n",
    "        for char in self.data.iloc[idx,0].strip():\n",
    "            indx = self.char_to_idx.get(char,0)\n",
    "            itm.append(indx)\n",
    "            l += 1\n",
    "            if l == MAX_LEN:\n",
    "                break\n",
    "        \n",
    "        if len(itm) < MAX_LEN:\n",
    "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
    "        \n",
    "        itm = torch.tensor(itm).long()\n",
    "        # To Differentiate Train and Test data\n",
    "        if self.data.shape[1] == 2:\n",
    "            label = self.data.iloc[idx,1]\n",
    "            return itm, label\n",
    "        else:\n",
    "            return itm, None\n",
    "\n",
    "train_dataset = NeuralNetDataLoader(train_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
    "val_dataset = NeuralNetDataLoader(val_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
    "test_dataset = NeuralNetDataLoader(test_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, embd_dim, pad_index):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.nhid = hidden_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings = input_dim, embedding_dim=embd_dim, padding_idx=pad_index)   \n",
    "        self.rnn = nn.GRU(embd_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embds = self.embedding(text).float()\n",
    "        embds = embds.permute(1,0,2)\n",
    "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = hidden.cuda() \n",
    "        out, hid = self.rnn(embds, hidden)       \n",
    "        out = self.dropout(out)\n",
    "        out = self.output(hid.squeeze(0))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(char_to_idx)\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = 1\n",
    "HIDDEN_DIM = 64\n",
    "EMBD_DIM = 128\n",
    "model = CharRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, EMBD_DIM, PAD_IDX)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharRNNTrainer = TrainingModule(model)\n",
    "CharRNNTrainer.train_model(train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_accuracy = CharRNNTrainer.evaluate(test_iter)\n",
    "print(\"Character Level Character Accuracy: {:4.4f}\".format(char_rnn_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
