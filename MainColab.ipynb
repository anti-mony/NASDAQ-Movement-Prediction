{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVb2bIl6l5Ms",
        "colab_type": "text"
      },
      "source": [
        "## **_Using News Data to Predict Movements in the Financial Movements_**\n",
        "\n",
        "We'll be using four apporaches here:\n",
        "\n",
        "* Continuous Bag of Words Model\n",
        "* Neural Network Model with Glove Word Embeddings\n",
        "* RNN Models using Word Embeddings\n",
        "* Character Level RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hBxcqvyl5Mu",
        "colab_type": "code",
        "outputId": "46b0d9f1-9b5e-45e6-ea77-35724abfe808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as tud\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import os, math\n",
        "import random\n",
        "import copy\n",
        "import string\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "\n",
        "from split_data import split_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK_z_yIbl5Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the random seeds so the experiments can be replicated exactly\n",
        "random.seed(72689)\n",
        "np.random.seed(72689)\n",
        "torch.manual_seed(72689)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(72689)\n",
        "\n",
        "# Global class labels.\n",
        "POS_LABEL = 'up'\n",
        "NEG_LABEL = 'down'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwxqx5eRl5M3",
        "colab_type": "text"
      },
      "source": [
        "**Reading in all the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Qj1Edbl5M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = pd.read_csv(\"ProcessedData/CombinedData.csv\")\n",
        "all_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "all_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFAaGjJUl5M7",
        "colab_type": "text"
      },
      "source": [
        "*Using a Small Subset of Data fro Development*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sON6smrxl5M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sample = all_data.sample(10000, random_state=68)\n",
        "data_sample.reset_index(drop=True, inplace=True)\n",
        "data_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7BNew3al5M_",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing the Data For Feeding Into The Model**\n",
        "\n",
        "Preprocessing Involves (in our case):\n",
        "* Turning All Words into lower/upper case, Normalization\n",
        "* removing punctuations, accent marks and other diacritics\n",
        "* removing stop words, sparse terms, and particular words\n",
        "* Lemmatize using NLTK (It's generally better than Stemming, but way slower)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7Wjrbfjl5NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing all Punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes any punctuation from the given string\n",
        "    \n",
        "    Args:\n",
        "        String: Input text to be cleaned\n",
        "        \n",
        "    Returns:\n",
        "        String with removed punctuation\n",
        "    \"\"\"\n",
        "    more_puncs = '—'+ '’'+ '“'+ '”'+ '…'\n",
        "    return text.translate(str.maketrans('', '', string.punctuation+more_puncs))\n",
        "\n",
        "# Removing all Stop Words\n",
        "def remove_stopwords(text, stop_words):\n",
        "    \"\"\"\n",
        "    Removes any stop words from the given string\n",
        "    \n",
        "    Args:\n",
        "        String: Input text to be cleaned\n",
        "        \n",
        "    Returns:\n",
        "        String with removed stop words\n",
        "    \"\"\"\n",
        "\n",
        "    text = word_tokenize(text)\n",
        "    return  \" \".join([i for i in text if i not in stop_words])\n",
        "\n",
        "def lemmetize(text, lemmatizer, pos_tag_dict):\n",
        "    \"\"\"\n",
        "    Lammatize the input string\n",
        "    \n",
        "    Args:\n",
        "        String: Input text to be Lammetized\n",
        "        Lemmatizer: NLTK WordNetLemmatizer Object\n",
        "        pos_tag_dict : Character to POS Type dictionary\n",
        "        \n",
        "    Returns:\n",
        "        String with removed punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    text = word_tokenize(text)\n",
        "    pos = nltk.pos_tag(text)\n",
        "    results = []\n",
        "    for pair in pos:\n",
        "        tag = pos_tag_dict.get(pair[1][0],wordnet.NOUN)\n",
        "        results.append(lemmatizer.lemmatize(pair[0], tag))\n",
        "        \n",
        "    return \" \".join(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7h3oPlsl5ND",
        "colab_type": "text"
      },
      "source": [
        "We choose only the columsn we want from the entire data set to pre-process. It speeds up the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdLHR7ifl5NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_data = all_data[['Content', 'CloseMove']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YYqUYc_l5NH",
        "colab_type": "text"
      },
      "source": [
        "The pre_process function below performs all the preprocessing we defined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ogUt9sPl5NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(df):\n",
        "    \n",
        "    \"\"\"\n",
        "    Preprocess the dataset passed to it\n",
        "    Performs the three types of pre processing described above\n",
        "    \n",
        "    Args:\n",
        "        df: Pandas data frame with 'Content' column\n",
        "    \n",
        "    Returns:\n",
        "        Pandas dataframe with pre-processed Content Column\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalization\n",
        "#     df['Title'] = df['Title'].str.lower()\n",
        "    df['Content'] = df['Content'].str.lower()\n",
        "\n",
        "    # Removing Punctuation\n",
        "#     df['Title'] = df['Title'].apply(remove_punctuation)\n",
        "    df['Content'] = df['Content'].apply(remove_punctuation)\n",
        "    \n",
        "    STOP_WORDS = set(stopwords.words('english'))\n",
        "    # Remove Stopwords\n",
        "#     df['Title'] = df['Title'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
        "    df['Content'] = df['Content'].apply(remove_stopwords, args=(STOP_WORDS, ))\n",
        "\n",
        "    # Lemmetization\n",
        "    lemmer = WordNetLemmatizer()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV\n",
        "               }\n",
        "#     df['Title'] = df['Title'].apply(lemmetize, args=(lemmer, tag_dict))\n",
        "    df['Content'] = df['Content'].apply(lemmetize, args=(lemmer, tag_dict))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BlU3p__l5NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTrTQsSml5NP",
        "colab_type": "text"
      },
      "source": [
        "**We run the pre_process function in parallel to make it faster using the Multi-Processing Module**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWamJa2Wl5NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processing in Parallel\n",
        "n_threads = mp.cpu_count()-1\n",
        "data_pieces = np.array_split(sub_data, n_threads)\n",
        "startTime = time.time()\n",
        "pool = mp.Pool(n_threads)\n",
        "data_sample = pd.concat(pool.map(pre_process, data_pieces))\n",
        "pool.close()\n",
        "pool.join()\n",
        "\n",
        "totalTime = time.time() - startTime\n",
        "print(\"Time taken in Pre-Processing: {}m {}s\".format(totalTime // 60, totalTime%60))\n",
        "data_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkDgP0TOl5NU",
        "colab_type": "text"
      },
      "source": [
        "**We drop the rows which exceed default python's csv field max limit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe6sKYqol5NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_max_len = 131072\n",
        "to_drop = []\n",
        "print\n",
        "for i in range(data_sample.shape[0]):\n",
        "    if len(data_sample.iloc[i,0]) >= csv_max_len-1:\n",
        "        to_drop.append(i)\n",
        "        \n",
        "data_sample.drop(to_drop, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx-bbwiAl5NY",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the Data and Storing it such that torch text can easily ingest it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXydA8mgl5NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 68\n",
        "split_data(df=data_sample,prefix='prod',seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4j9jM3vl5Nc",
        "colab_type": "text"
      },
      "source": [
        "## **Preparing Data**\n",
        "* Building the Vocabulary (Using Spacy) | **MAX_VOCAB_SIZE** = 70000\n",
        "* Splitting the data for Test and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jab1zOjpl5Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrGjfEil5Nf",
        "colab_type": "text"
      },
      "source": [
        "### Reading in Data Using TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CRj_9hQl5Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can choose what data to read in Development/Production with the help of the switch below\n",
        "# 0: Production\n",
        "# 1: Development\n",
        "\n",
        "dev_prod_sel = {1:\"dev\", 0:\"prod\"}\n",
        "prefix = dev_prod_sel[0]\n",
        "train, val, test = data.TabularDataset.splits(\n",
        "        path='./ProcessedData/', train=prefix+'_train.csv',\n",
        "        validation=prefix+'_val.csv', test=prefix+'_test.csv', format='csv',\n",
        "        fields=[('Text', TEXT), ('Label', LABEL)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79zswBHRl5Ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 70000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13Yz31Tl5Nl",
        "colab_type": "text"
      },
      "source": [
        "## **Bag Of Words Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Qg6C7Xl5Nl",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0t4PLZml5Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class BOWDataLoader(tud.Dataset):\n",
        "    def __init__(self, data, vocab_size, text, field):\n",
        "        '''\n",
        "        Initialize the Bag of Words Data Loader\n",
        "        Build the Dictionary and store the data\n",
        "        '''\n",
        "        self.vocab_size = vocab_size\n",
        "        self.TEXT = text\n",
        "        self.LABEL = field\n",
        "        self.TEXT.build_vocab(data, max_size = vocab_size)\n",
        "        self.LABEL.build_vocab(data)\n",
        "        self.data = data\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return len(self.data.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        itm = torch.zeros(self.vocab_size+2)\n",
        "        for word in self.data[idx].Text:\n",
        "            itm[self.TEXT.vocab.stoi[word]] += 1\n",
        "        \n",
        "        # To Differentiate Train and Test data\n",
        "        if len(self.data.fields) == 2:\n",
        "            label = self.data[idx].Label\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "# Creating three datasets using the Dataloader class defined above for training, validation and testing.\n",
        "train_dataset = BOWDataLoader(train, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
        "val_dataset = BOWDataLoader(val, MAX_VOCAB_SIZE, TEXT, LABEL)\n",
        "test_dataset = BOWDataLoader(test, MAX_VOCAB_SIZE, TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjOMzDkHl5No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqex32igl5Nr",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words Model Training Module\n",
        "\n",
        "Here we define the training and evaluation functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ukcDm0Tl5Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BOWTrainingModule():\n",
        "    '''\n",
        "    Training Module helps us train the Bag of Words Model\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, model, batch_size):\n",
        "        \n",
        "        \"\"\"\n",
        "        Initialize the BOW Training Module.\n",
        "        This module containt methods for training and evaluation of the model passed.\n",
        "        \n",
        "        Args:\n",
        "            model: Bag of Words Model Class Object\n",
        "            batch_size: Nume of examples to process together\n",
        "        \n",
        "        Returns:\n",
        "            BOW Training Object\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        # Batch Size\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Cuda Availability\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "                \n",
        "        # Loss Function\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    def train_epoch(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains a logistic regression model across all examples in the dataset.\n",
        "        \n",
        "        Args:\n",
        "            dataset: The training dataset of BOWDataLoader type\n",
        "        \"\"\"\n",
        "        self.dataloader = tud.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.model.train()\n",
        "        for i, (X,y) in enumerate(self.dataloader):\n",
        "            X = X.float()\n",
        "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).long()\n",
        "            if self.cuda:\n",
        "                X  = X.cuda()\n",
        "                y = y.cuda()\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            predictions = self.model.forward(X)\n",
        "            \n",
        "            loss = self.loss_fn(predictions, y)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            if (i) % 30 == 0:\n",
        "                print(\"Iteration : {:4d} | Loss : {:4.4f}\".format(i, loss.item()))\n",
        "            \n",
        "            self.optimizer.step()\n",
        "        \n",
        "    def train_model(self, train_data, val_data, num_epocs = 5):\n",
        "        \"\"\"\n",
        "        Trains the model and saves the best model according to the validation score\n",
        "        \n",
        "        Args:\n",
        "            train_data: The training dataset of BOWDataLoader type\n",
        "            val_data: The validation dataset of BOWDataLoader type\n",
        "            num_epocs: Number of epocs you want to train the model for (Default: 2)\n",
        "        \n",
        "        Returns:\n",
        "            BOWClassifier: The best model according to the validation scores.\n",
        "            \n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        accuracy = [0.]\n",
        "        for epoch in range(num_epocs):\n",
        "            self.train_epoch(train_data)\n",
        "            val_accuracy = self.evaluate(val_data)\n",
        "            train_accuracy = self.evaluate(train_data)\n",
        "            print(\"Validation Accuracy: {:4.4f} | Train Accuracy: {:4.4f}\".format(val_accuracy, train_accuracy))\n",
        "            if val_accuracy > max(accuracy):\n",
        "                best_model = copy.deepcopy(self.model)        \n",
        "            accuracy.append(val_accuracy)\n",
        "        \n",
        "        return best_model\n",
        "                \n",
        "    def evaluate(self, data):\n",
        "        '''\n",
        "        Evaluate the current model with the dataset provided\n",
        "        \n",
        "        Args:\n",
        "            data: Any dataset of type BOWDataLoader type\n",
        "        \n",
        "        Returns: \n",
        "            Float: Accuracy of the model on the given data set\n",
        "        '''\n",
        "        self.model.eval()\n",
        "        dataloader = tud.DataLoader(data, batch_size=self.batch_size, shuffle=False)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for i, (X,y) in enumerate(dataloader):\n",
        "            X = X.float()\n",
        "            if self.cuda:\n",
        "                X = X.cuda()\n",
        "            predictions = self.model.forward(X).max(1)[1].cpu().numpy().reshape(-1)\n",
        "            correct += (predictions == np.asarray(y, dtype=np.float64)).sum()\n",
        "            total += predictions.shape[0]\n",
        "        \n",
        "        return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MiiuV2el5Nx",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg88rHDl5N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BOWClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Constructing a Logistic Regression Model\n",
        "        \"\"\"\n",
        "        super(BOWClassifier, self).__init__()\n",
        "        \n",
        "        # Linear layer\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Passes the data through the network and return the output\n",
        "        \"\"\"\n",
        "        result = self.fc(text)\n",
        "        return (result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THPqbJEtl5N3",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSFoV7ogl5N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = MAX_VOCAB_SIZE + 2\n",
        "OUTPUT_DIM = 2\n",
        "BATCH_SIZE = 64\n",
        "bow_model = BOWClassifier(INPUT_DIM, OUTPUT_DIM)\n",
        "if torch.cuda.is_available():\n",
        "    bow_model = bow_model.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmWFF-33l5N8",
        "colab_type": "text"
      },
      "source": [
        "### Training the BOW Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQUI4JtWl5N9",
        "colab_type": "code",
        "outputId": "1b16e45b-70e6-4dd7-df1d-514a8c7175e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bow_trainer = BOWTrainingModule(bow_model, BATCH_SIZE)\n",
        "best_bow_model = bow_trainer.train_model(train_dataset, val_dataset)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration :    0 | Loss : 0.6967\n",
            "Iteration :   30 | Loss : 0.6652\n",
            "Iteration :   60 | Loss : 0.7312\n",
            "Iteration :   90 | Loss : 0.7017\n",
            "Iteration :  120 | Loss : 0.7359\n",
            "Iteration :  150 | Loss : 0.7391\n",
            "Iteration :  180 | Loss : 0.7065\n",
            "Iteration :  210 | Loss : 0.8727\n",
            "Iteration :  240 | Loss : 0.6869\n",
            "Iteration :  270 | Loss : 0.6868\n",
            "Iteration :  300 | Loss : 0.7939\n",
            "Iteration :  330 | Loss : 0.6455\n",
            "Iteration :  360 | Loss : 0.6414\n",
            "Iteration :  390 | Loss : 0.7152\n",
            "Iteration :  420 | Loss : 0.6762\n",
            "Iteration :  450 | Loss : 0.6898\n",
            "Iteration :  480 | Loss : 0.8547\n",
            "Validation Accuracy: 0.5379 | Train Accuracy: 0.7595\n",
            "Iteration :    0 | Loss : 0.6325\n",
            "Iteration :   30 | Loss : 0.5087\n",
            "Iteration :   60 | Loss : 0.4688\n",
            "Iteration :   90 | Loss : 0.4370\n",
            "Iteration :  120 | Loss : 0.4150\n",
            "Iteration :  150 | Loss : 0.4888\n",
            "Iteration :  180 | Loss : 0.5377\n",
            "Iteration :  210 | Loss : 0.5618\n",
            "Iteration :  240 | Loss : 0.5715\n",
            "Iteration :  270 | Loss : 0.5104\n",
            "Iteration :  300 | Loss : 0.5599\n",
            "Iteration :  330 | Loss : 0.5143\n",
            "Iteration :  360 | Loss : 0.6401\n",
            "Iteration :  390 | Loss : 0.5130\n",
            "Iteration :  420 | Loss : 0.5573\n",
            "Iteration :  450 | Loss : 0.5287\n",
            "Iteration :  480 | Loss : 0.6618\n",
            "Validation Accuracy: 0.5606 | Train Accuracy: 0.8406\n",
            "Iteration :    0 | Loss : 0.4319\n",
            "Iteration :   30 | Loss : 0.3605\n",
            "Iteration :   60 | Loss : 0.3862\n",
            "Iteration :   90 | Loss : 0.3852\n",
            "Iteration :  120 | Loss : 0.3992\n",
            "Iteration :  150 | Loss : 0.3725\n",
            "Iteration :  180 | Loss : 0.5595\n",
            "Iteration :  210 | Loss : 0.4811\n",
            "Iteration :  240 | Loss : 0.4260\n",
            "Iteration :  270 | Loss : 0.4718\n",
            "Iteration :  300 | Loss : 0.5310\n",
            "Iteration :  330 | Loss : 0.4824\n",
            "Iteration :  360 | Loss : 0.3899\n",
            "Iteration :  390 | Loss : 0.6151\n",
            "Iteration :  420 | Loss : 0.3919\n",
            "Iteration :  450 | Loss : 0.4237\n",
            "Iteration :  480 | Loss : 0.4142\n",
            "Validation Accuracy: 0.5649 | Train Accuracy: 0.8802\n",
            "Iteration :    0 | Loss : 0.2924\n",
            "Iteration :   30 | Loss : 0.3636\n",
            "Iteration :   60 | Loss : 0.3338\n",
            "Iteration :   90 | Loss : 0.3653\n",
            "Iteration :  120 | Loss : 0.3919\n",
            "Iteration :  150 | Loss : 0.4762\n",
            "Iteration :  180 | Loss : 0.4808\n",
            "Iteration :  210 | Loss : 0.3474\n",
            "Iteration :  240 | Loss : 0.3870\n",
            "Iteration :  270 | Loss : 0.3607\n",
            "Iteration :  300 | Loss : 0.3837\n",
            "Iteration :  330 | Loss : 0.3908\n",
            "Iteration :  360 | Loss : 0.4885\n",
            "Iteration :  390 | Loss : 0.3338\n",
            "Iteration :  420 | Loss : 0.3851\n",
            "Iteration :  450 | Loss : 0.3919\n",
            "Iteration :  480 | Loss : 0.4275\n",
            "Validation Accuracy: 0.5613 | Train Accuracy: 0.9007\n",
            "Iteration :    0 | Loss : 0.3470\n",
            "Iteration :   30 | Loss : 0.3002\n",
            "Iteration :   60 | Loss : 0.2982\n",
            "Iteration :   90 | Loss : 0.5496\n",
            "Iteration :  120 | Loss : 0.3571\n",
            "Iteration :  150 | Loss : 0.3590\n",
            "Iteration :  180 | Loss : 0.3287\n",
            "Iteration :  210 | Loss : 0.4548\n",
            "Iteration :  240 | Loss : 0.3202\n",
            "Iteration :  270 | Loss : 0.3059\n",
            "Iteration :  300 | Loss : 0.3964\n",
            "Iteration :  330 | Loss : 0.3191\n",
            "Iteration :  360 | Loss : 0.4060\n",
            "Iteration :  390 | Loss : 0.3848\n",
            "Iteration :  420 | Loss : 0.4528\n",
            "Iteration :  450 | Loss : 0.3671\n",
            "Iteration :  480 | Loss : 0.6733\n",
            "Validation Accuracy: 0.5702 | Train Accuracy: 0.9093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G416-KN1l5OA",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMLmgXll5OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "316a3f02-31f1-400c-b522-5fabb4fdea96"
      },
      "source": [
        "bow_accuracy = BOWTrainingModule(best_bow_model, BATCH_SIZE).evaluate(test_dataset)\n",
        "print(\"Bag Of Words Model Accuracy : {:4.4f}\".format(bow_accuracy))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag Of Words Model Accuracy : 0.5678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNS6Eipl5OG",
        "colab_type": "text"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rna2EubJl5OG",
        "colab_type": "text"
      },
      "source": [
        "## **Glove Embeddings**\n",
        "\n",
        "We use Glove Embeddings throughout the notebook. Below are a few functions that help us load and transform the Glove Encodings as we want.\n",
        "\n",
        "**Refrence:**\n",
        "\n",
        "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C4JvWYgl5OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove(path_file):\n",
        "    \"\"\"\n",
        "    Loads the Glove Pre-Trained Embeddings\n",
        "    \n",
        "    Args:\n",
        "        path_file: Path to the official glove embedding text file\n",
        "    \n",
        "    Returns: Dictionary {Word: [Embedding]}\n",
        "    \n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    print(\"Loading Glove Model ...\")\n",
        "    glove = {}\n",
        "    with open(path_file) as f:\n",
        "        for line in f:\n",
        "            tmp = line.split()\n",
        "            glove[tmp[0]] = np.asarray(tmp[1:], dtype=np.float64)\n",
        "    print(\"Glove Model Loaded in {} s\".format(time.time()-start_time))\n",
        "    return glove\n",
        "\n",
        "def gloveWordIndex(glove):\n",
        "    \"\"\"\n",
        "    Generates word to index mappings\n",
        "    0 --> <unk>\n",
        "    1 --> <pad>\n",
        "    Args:\n",
        "        Loaded Glove Model as a dict\n",
        "        \n",
        "    Returns:\n",
        "        word to index map {word:idx} and index to word map{idx:word}\n",
        "    \n",
        "    \"\"\"\n",
        "    w_i = {k:v+2 for v,k in enumerate(glove.keys())}\n",
        "    w_i['<unk>'] = 0\n",
        "    w_i['<pad>'] = 1\n",
        "    i_w = {v+2:k for v,k in enumerate(glove.keys())}\n",
        "    i_w[0] = '<unk>'\n",
        "    i_w[1] = '<pad>'\n",
        "    return w_i, i_w\n",
        "\n",
        "def getWeightMatrix(glove):\n",
        "    \"\"\"\n",
        "    Generates the embedding matrix from the Glove dictionary\n",
        "    \n",
        "    Args:\n",
        "        Glove: Glove embedding dictionary as returned by the function 'load_glove' defined above.\n",
        "    \n",
        "    Returns:\n",
        "        Numpy Array: Embedding Matrix\n",
        "    \"\"\"\n",
        "    embd_dim = glove['a'].shape[0]\n",
        "    num_embeddings = len(glove.keys())\n",
        "    w_m = np.zeros((num_embeddings+2, embd_dim))\n",
        "    w_m[0] = np.random.rand(embd_dim)\n",
        "    w_m[1] = np.zeros(embd_dim)\n",
        "    for i, word in enumerate(glove.keys()):\n",
        "        w_m[i+2] = glove[word]\n",
        "    \n",
        "    return w_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDicxqdil5OK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "935fef0a-4cec-479c-9524-c53dacd8c90f"
      },
      "source": [
        "glove = load_glove(\"Embeddings/glove.6B.100d.txt\")\n",
        "word_to_idx, idx_to_word = gloveWordIndex(glove)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model ...\n",
            "Glove Model Loaded in 11.144344806671143 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyUCVzCVl5ON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa2e6f08-4a6e-4b45-a08e-e240fca85d38"
      },
      "source": [
        "weight_matrix = getWeightMatrix(glove)\n",
        "weight_matrix.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400002, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j41REw2ll5OP",
        "colab_type": "text"
      },
      "source": [
        "--------\n",
        "## **Data Loader**\n",
        "\n",
        "We set up the data loader to pad the sequqnces and return us sequences of length 1200. If longer then trim them to 1200 words.\n",
        "\n",
        "We can also use pad-packed-sequence functions from PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96zkq7u_l5OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = len(glove.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NonEqR4il5OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class EmbedDataLoader(tud.Dataset):\n",
        "    def __init__(self, data, word_to_idx, idx_to_word, vocab_size):\n",
        "        \"\"\"\n",
        "        Dataloader for the models using Embeddings\n",
        "        \n",
        "        Args:\n",
        "            data: Dataset for the data loader (TorchText Object)\n",
        "            word_to_idx: Mapping from word to Indices\n",
        "            idx_to_word: Mapping from Indices to words\n",
        "            vocab_size: Maximum Vocabulary Size\n",
        "            \n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_Word = idx_to_word\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return len(self.data.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        MAX_LEN = 1200\n",
        "        itm = []\n",
        "        l = 0\n",
        "        for word in self.data[idx].Text:\n",
        "            indx = self.word_to_idx.get(word,0)\n",
        "            itm.append(indx)\n",
        "            l += 1\n",
        "            if l == MAX_LEN:\n",
        "                break\n",
        "        \n",
        "        if len(itm) < MAX_LEN:\n",
        "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
        "        \n",
        "        itm = torch.tensor(itm).long()\n",
        "        # To Differentiate Train and Test data\n",
        "        if len(self.data.fields) == 2:\n",
        "            label = self.data[idx].Label\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "\n",
        "# Creating three datasets using the Dataloader class defined above for training, validation and testing.\n",
        "train_dataset = EmbedDataLoader(train, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
        "val_dataset = EmbedDataLoader(val, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)\n",
        "test_dataset = EmbedDataLoader(test, word_to_idx, idx_to_word, MAX_VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow3d3Nyul5OW",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the Data Iterators\n",
        "\n",
        "Using the data loader we set-up above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUqyubTLl5OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HvSB3c3l5OZ",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "## **Training Module**\n",
        "This module contains the evaluate and training functios.\n",
        "\n",
        "This module will help train us all the future models we make"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOCfVy9-l5OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainingModule():\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        '''\n",
        "        Training Module for the rest of the models in the Notebook\n",
        "        \n",
        "        Args:\n",
        "            Model: The Model you want to train\n",
        "        \n",
        "        '''\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.cuda = torch.cuda.is_available()\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        \n",
        "    def train_epoch(self, iterator):\n",
        "        \"\"\"\n",
        "        Trains the model over the entire dataset provided once.\n",
        "        \n",
        "        Args:\n",
        "            iterator: Training Dataset Iterator\n",
        "        \n",
        "        Returns:\n",
        "            Tuple(Float, Float): Loss and Accuracy over the entire data set\n",
        "        \"\"\"\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        self.model.train()\n",
        "        for i, (X,y) in enumerate(iterator):\n",
        "            self.optimizer.zero_grad()\n",
        "            X = X.long()\n",
        "            y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
        "            if self.cuda:\n",
        "                X = X.cuda()\n",
        "                y = y.cuda()\n",
        "            preds = self.model.forward(X).squeeze(1)\n",
        "            \n",
        "            loss = self.loss_fn(preds, y)\n",
        "            \n",
        "            acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
        "            if i % 30 == 0:\n",
        "                print(\"Iteration: {:4d} | Loss : {:4.4f} | Accuracy : {:4.4f}\".format(i, loss.item(), acc))\n",
        "                                \n",
        "            loss.backward()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc\n",
        "            \n",
        "            self.optimizer.step()\n",
        "        \n",
        "        return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
        "    \n",
        "        \n",
        "    def train_model(self, train_iterator, dev_iterator, num_epocs = 5):\n",
        "        \"\"\"\n",
        "        Trains the model over the trianing data provided and \n",
        "        chooses the one which best performs on the validation set\n",
        "        \n",
        "        Args:\n",
        "            train_iterator: Iterator over the training Dataset\n",
        "            dev_iterator: Iterator over the validation set\n",
        "            num_epocs: Number of Epocs you want to train the model\n",
        "            \n",
        "        Returns:\n",
        "            Model: Best model according to the performance in the validation data set\n",
        "            \n",
        "        \"\"\"\n",
        "        val_acc = [0.]\n",
        "        for epoch in range(num_epocs):\n",
        "            ep_loss, ep_accu = self.train_epoch(train_iterator)\n",
        "            dev_acc = self.evaluate(dev_iterator)\n",
        "            train_acc = self.evaluate(train_iterator)\n",
        "            print(\"Val. Loss : {:4.4f} | Val. Accuracy : {:4.4f} | Train. Accuracy: {:4.4f}\".format(dev_acc[0], dev_acc[1], train_acc[1]))\n",
        "            if dev_acc[1] > max(val_acc):\n",
        "                best_model = copy.deepcopy(self.model)\n",
        "            val_acc.append(dev_acc[1])\n",
        "\n",
        "        return best_model\n",
        "        \n",
        "    \n",
        "    def evaluate(self, iterator):\n",
        "        \"\"\"\n",
        "        Evaluates the model with the given dataset\n",
        "        \n",
        "        Args:\n",
        "            iterator: dataset iterator\n",
        "            \n",
        "        Returns:\n",
        "            Tuple(Float, Float) : Model Loss, Model Accuracy on the given dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        epoch_loss  = 0\n",
        "        epoch_acc = 0\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (X,y) in enumerate(iterator):\n",
        "                X = X.long()\n",
        "                y = torch.Tensor(np.asarray(y, dtype=np.float64)).float()\n",
        "                if self.cuda:\n",
        "                    X = X.cuda()\n",
        "                    y = y.cuda()\n",
        "                preds = self.model.forward(X).squeeze(1)\n",
        "\n",
        "                loss = self.loss_fn(preds, y)\n",
        "                \n",
        "                acc = (torch.round(torch.sigmoid(preds))==y).sum().item()/y.shape[0]\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc\n",
        "        \n",
        "        return epoch_loss/len(iterator), epoch_acc/len(iterator)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOjOoiel5Ob",
        "colab_type": "text"
      },
      "source": [
        "## **Neural Network based Model with Word Embeddings**\n",
        "\n",
        "We use a Neural Network now with Word Embeddings, whoose :\n",
        "* Input : A sentence\n",
        "* Output: Label : {UP, DOWN}\n",
        "\n",
        "The basic structure of a model class is as above. Functions like classify, evaluate and train will be defined along with pretrained word-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOmUGJKyl5Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetClassifier(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Neural Network Model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, pad_index, embedding_weights):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # We start the embeddings from a pre-trained vector\n",
        "        embd_dim = embedding_weights.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)\n",
        "        \n",
        "        hid_dim1 = 64\n",
        "        hid_dim2 = 32\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "        \n",
        "        self.hd1 = nn.Linear(embd_dim, hid_dim1)\n",
        "        self.hd2 = nn.Linear(hid_dim1, hid_dim2)\n",
        "        self.out = nn.Linear(hid_dim2, output_dim)\n",
        "        \n",
        "        self.activate = nn.ReLU()\n",
        "    \n",
        "    def forward(self,text):\n",
        "        \n",
        "#         print(\"Text: \", text.shape)\n",
        "        embds = self.embedding(text)\n",
        "#         print(\"Embds: \", embds.shape)\n",
        "        mean_embd = torch.mean(embds, 1)\n",
        "#         print(\"Embedding:\", mean_embd.shape)\n",
        "        output = self.activate(self.hd1(mean_embd.float()))\n",
        "#         print(\"Layer 1: \",output.shape)\n",
        "        output = self.drop_out(output)\n",
        "        output = self.activate(self.hd2(output))\n",
        "        output = self.drop_out(output)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE3eMUsel5Oe",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Neural Net Model \n",
        "with the appropriate dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koFWrHSHl5Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = weight_matrix.shape[0]\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "nn_model = NeuralNetClassifier(INPUT_DIM, OUTPUT_DIM, PAD_IDX, weight_matrix)\n",
        "if torch.cuda.is_available():\n",
        "    nn_model = nn_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa7eyysRl5Og",
        "colab_type": "text"
      },
      "source": [
        "### Training the Neural Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LD8vaErl5Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebd759bb-3daa-46c9-bbaf-40b8d47af687"
      },
      "source": [
        "neural_trainer = TrainingModule(nn_model)\n",
        "best_nn_model = neural_trainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.6905 | Accuracy : 0.5469\n",
            "Iteration:   30 | Loss : 0.6798 | Accuracy : 0.6875\n",
            "Iteration:   60 | Loss : 0.6461 | Accuracy : 0.6875\n",
            "Iteration:   90 | Loss : 0.6635 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6888 | Accuracy : 0.5781\n",
            "Iteration:  150 | Loss : 0.6672 | Accuracy : 0.6719\n",
            "Iteration:  180 | Loss : 0.6793 | Accuracy : 0.5625\n",
            "Iteration:  210 | Loss : 0.6579 | Accuracy : 0.6562\n",
            "Iteration:  240 | Loss : 0.6771 | Accuracy : 0.6406\n",
            "Iteration:  270 | Loss : 0.6945 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6829 | Accuracy : 0.5781\n",
            "Iteration:  330 | Loss : 0.6818 | Accuracy : 0.6250\n",
            "Iteration:  360 | Loss : 0.6732 | Accuracy : 0.6406\n",
            "Iteration:  390 | Loss : 0.6963 | Accuracy : 0.5469\n",
            "Iteration:  420 | Loss : 0.6723 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6867 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6525 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6755 | Val. Accuracy : 0.5913 | Train. Accuracy: 0.5932\n",
            "Iteration:    0 | Loss : 0.7261 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.6874 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6274 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6675 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6884 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6456 | Accuracy : 0.6719\n",
            "Iteration:  180 | Loss : 0.7048 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6424 | Accuracy : 0.6562\n",
            "Iteration:  240 | Loss : 0.6669 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7061 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6783 | Accuracy : 0.5938\n",
            "Iteration:  330 | Loss : 0.6700 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6577 | Accuracy : 0.6406\n",
            "Iteration:  390 | Loss : 0.6858 | Accuracy : 0.5469\n",
            "Iteration:  420 | Loss : 0.6680 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6809 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6443 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6746 | Val. Accuracy : 0.5913 | Train. Accuracy: 0.5932\n",
            "Iteration:    0 | Loss : 0.7609 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.6841 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6095 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6584 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6744 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6304 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.7092 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6443 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6636 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7058 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6653 | Accuracy : 0.5938\n",
            "Iteration:  330 | Loss : 0.6601 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6203 | Accuracy : 0.6406\n",
            "Iteration:  390 | Loss : 0.6905 | Accuracy : 0.5469\n",
            "Iteration:  420 | Loss : 0.6467 | Accuracy : 0.6250\n",
            "Iteration:  450 | Loss : 0.6765 | Accuracy : 0.5938\n",
            "Iteration:  480 | Loss : 0.6441 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6736 | Val. Accuracy : 0.5914 | Train. Accuracy: 0.5937\n",
            "Iteration:    0 | Loss : 0.7663 | Accuracy : 0.4062\n",
            "Iteration:   30 | Loss : 0.6695 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6070 | Accuracy : 0.7656\n",
            "Iteration:   90 | Loss : 0.6795 | Accuracy : 0.6250\n",
            "Iteration:  120 | Loss : 0.6811 | Accuracy : 0.5312\n",
            "Iteration:  150 | Loss : 0.6086 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.6603 | Accuracy : 0.5156\n",
            "Iteration:  210 | Loss : 0.6072 | Accuracy : 0.6875\n",
            "Iteration:  240 | Loss : 0.6110 | Accuracy : 0.7500\n",
            "Iteration:  270 | Loss : 0.6918 | Accuracy : 0.5938\n",
            "Iteration:  300 | Loss : 0.6734 | Accuracy : 0.6094\n",
            "Iteration:  330 | Loss : 0.6299 | Accuracy : 0.6250\n",
            "Iteration:  360 | Loss : 0.6055 | Accuracy : 0.6562\n",
            "Iteration:  390 | Loss : 0.6056 | Accuracy : 0.6719\n",
            "Iteration:  420 | Loss : 0.6438 | Accuracy : 0.6562\n",
            "Iteration:  450 | Loss : 0.6168 | Accuracy : 0.7031\n",
            "Iteration:  480 | Loss : 0.6255 | Accuracy : 0.6719\n",
            "Val. Loss : 0.6823 | Val. Accuracy : 0.5820 | Train. Accuracy: 0.6862\n",
            "Iteration:    0 | Loss : 0.7291 | Accuracy : 0.4688\n",
            "Iteration:   30 | Loss : 0.6902 | Accuracy : 0.6094\n",
            "Iteration:   60 | Loss : 0.5636 | Accuracy : 0.7500\n",
            "Iteration:   90 | Loss : 0.5825 | Accuracy : 0.6875\n",
            "Iteration:  120 | Loss : 0.6780 | Accuracy : 0.6875\n",
            "Iteration:  150 | Loss : 0.4849 | Accuracy : 0.8125\n",
            "Iteration:  180 | Loss : 0.6179 | Accuracy : 0.6406\n",
            "Iteration:  210 | Loss : 0.5489 | Accuracy : 0.7812\n",
            "Iteration:  240 | Loss : 0.5211 | Accuracy : 0.7812\n",
            "Iteration:  270 | Loss : 0.6234 | Accuracy : 0.5781\n",
            "Iteration:  300 | Loss : 0.6149 | Accuracy : 0.6562\n",
            "Iteration:  330 | Loss : 0.5634 | Accuracy : 0.6875\n",
            "Iteration:  360 | Loss : 0.5247 | Accuracy : 0.7500\n",
            "Iteration:  390 | Loss : 0.4580 | Accuracy : 0.8125\n",
            "Iteration:  420 | Loss : 0.5643 | Accuracy : 0.6562\n",
            "Iteration:  450 | Loss : 0.6260 | Accuracy : 0.6250\n",
            "Iteration:  480 | Loss : 0.5289 | Accuracy : 0.7656\n",
            "Val. Loss : 0.7550 | Val. Accuracy : 0.5750 | Train. Accuracy: 0.7907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBz3CO_Ol5Ok",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13WlvmgHl5Ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8095b1d3-17a7-4fc9-f444-c285d34ecadf"
      },
      "source": [
        "neural_accuracy = TrainingModule(best_nn_model).evaluate(test_iter)\n",
        "print(\"Neural Network Model Accuracy : {:4.4f}\".format(neural_accuracy[1]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network Model Accuracy : 0.6083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46tyVxqil5On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGE7LIoil5Op",
        "colab_type": "text"
      },
      "source": [
        "## **Recurrent Neural Network (GRU) with Glove Embeddings**\n",
        "\n",
        "We use GRU as a RNN model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bacKh9pl5Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordRNNClassifier(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Word Level GRU RNN Model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, pad_index, embedding_weights, drop_out = 0):\n",
        "        \n",
        "        super().__init__()\n",
        "        embd_dim = embedding_weights.shape[1]\n",
        "        self.nhid = hidden_dim\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights), freeze=False, padding_idx = pad_index)   \n",
        "        self.rnn = nn.GRU(embd_dim, hidden_dim, dropout=drop_out)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, text):\n",
        "        \n",
        "        embds = self.embedding(text).float()\n",
        "        embds = embds.permute(1,0,2)\n",
        "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
        "        if torch.cuda.is_available():\n",
        "            hidden = hidden.cuda() \n",
        "        out, hid = self.rnn(embds, hidden)    \n",
        "        hid = self.dropout(hid)\n",
        "        out = self.output(hid.squeeze(0))\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C59_T6ehl5Or",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the RNN (GRU) Model\n",
        "\n",
        "with correct parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCo138Zrl5Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = weight_matrix.shape[0]\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "HIDDEN_DIM = 64\n",
        "rnn_model = WordRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, PAD_IDX, weight_matrix)\n",
        "if torch.cuda.is_available():\n",
        "    rnn_model = rnn_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fok-yErYl5Ov",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JNx9EOCl5Ow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d536baf5-f027-4e39-f1a7-38a4a7eee712"
      },
      "source": [
        "RNN_Trainer = TrainingModule(rnn_model)\n",
        "best_rnn_model = RNN_Trainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.6853 | Accuracy : 0.5625\n",
            "Iteration:   30 | Loss : 0.6836 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6133 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6464 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6877 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6503 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.6946 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6556 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6664 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7059 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6750 | Accuracy : 0.5938\n",
            "Iteration:  330 | Loss : 0.6705 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6555 | Accuracy : 0.6562\n",
            "Iteration:  390 | Loss : 0.6870 | Accuracy : 0.5469\n",
            "Iteration:  420 | Loss : 0.6808 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6627 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6514 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6773 | Val. Accuracy : 0.5907 | Train. Accuracy: 0.5943\n",
            "Iteration:    0 | Loss : 0.7317 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.6743 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6052 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6546 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6915 | Accuracy : 0.5312\n",
            "Iteration:  150 | Loss : 0.6385 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.6998 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6471 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6623 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7064 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6755 | Accuracy : 0.6094\n",
            "Iteration:  330 | Loss : 0.6680 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6614 | Accuracy : 0.6562\n",
            "Iteration:  390 | Loss : 0.6960 | Accuracy : 0.5469\n",
            "Iteration:  420 | Loss : 0.6893 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6686 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6583 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6779 | Val. Accuracy : 0.5902 | Train. Accuracy: 0.6001\n",
            "Iteration:    0 | Loss : 0.7399 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.6737 | Accuracy : 0.6094\n",
            "Iteration:   60 | Loss : 0.6002 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6495 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6900 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6482 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.7012 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6542 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6634 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7049 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6596 | Accuracy : 0.6094\n",
            "Iteration:  330 | Loss : 0.6630 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6410 | Accuracy : 0.6562\n",
            "Iteration:  390 | Loss : 0.6883 | Accuracy : 0.5625\n",
            "Iteration:  420 | Loss : 0.6983 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6540 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6565 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6817 | Val. Accuracy : 0.5889 | Train. Accuracy: 0.6055\n",
            "Iteration:    0 | Loss : 0.7554 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.6777 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6022 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6564 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6855 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6454 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.6843 | Accuracy : 0.5625\n",
            "Iteration:  210 | Loss : 0.6565 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6695 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.6937 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6342 | Accuracy : 0.6406\n",
            "Iteration:  330 | Loss : 0.6559 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6444 | Accuracy : 0.6406\n",
            "Iteration:  390 | Loss : 0.6815 | Accuracy : 0.5625\n",
            "Iteration:  420 | Loss : 0.7054 | Accuracy : 0.5781\n",
            "Iteration:  450 | Loss : 0.6488 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6520 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6886 | Val. Accuracy : 0.5880 | Train. Accuracy: 0.6076\n",
            "Iteration:    0 | Loss : 0.7192 | Accuracy : 0.4531\n",
            "Iteration:   30 | Loss : 0.7047 | Accuracy : 0.5781\n",
            "Iteration:   60 | Loss : 0.5962 | Accuracy : 0.7812\n",
            "Iteration:   90 | Loss : 0.6580 | Accuracy : 0.6406\n",
            "Iteration:  120 | Loss : 0.6829 | Accuracy : 0.5469\n",
            "Iteration:  150 | Loss : 0.6394 | Accuracy : 0.7031\n",
            "Iteration:  180 | Loss : 0.6792 | Accuracy : 0.5625\n",
            "Iteration:  210 | Loss : 0.6498 | Accuracy : 0.6719\n",
            "Iteration:  240 | Loss : 0.6510 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.6897 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6313 | Accuracy : 0.6406\n",
            "Iteration:  330 | Loss : 0.6590 | Accuracy : 0.6094\n",
            "Iteration:  360 | Loss : 0.6242 | Accuracy : 0.6562\n",
            "Iteration:  390 | Loss : 0.6808 | Accuracy : 0.5625\n",
            "Iteration:  420 | Loss : 0.6850 | Accuracy : 0.5938\n",
            "Iteration:  450 | Loss : 0.6408 | Accuracy : 0.6094\n",
            "Iteration:  480 | Loss : 0.6599 | Accuracy : 0.6406\n",
            "Val. Loss : 0.6941 | Val. Accuracy : 0.5881 | Train. Accuracy: 0.6077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUcJk6xl5Oy",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmVnIi4Ol5Oy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "3748d62d-c283-402b-f343-f0c9196a9104"
      },
      "source": [
        "rnn_accuracy = TrainingModule(best_rnn_model).evaluate(test_iter)\n",
        "print(\"RNN Model Accuracy : {:4.4f}\".format(rnn_accuracy[1]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RNN Model Accuracy : 0.6079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lzwRNvNl5O0",
        "colab_type": "text"
      },
      "source": [
        "## **Chacracter Level RNN Model** _With Letter Embeddings_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7kKonSEl5O1",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader\n",
        "\n",
        "Here we need to do something different. We don't want to Normalize the data, remove punctuation or any lemmetization. We want the model to learn how all the differene characters work together and relate to each other. So we will manually create our own mappings from index to letters and use them in the data loader.\n",
        "\n",
        "Also, we won't be using TorchText here, we'll just be using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CQwXUAl5O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data, val_data, test_data = split_data(df = data_sample[['Content', 'CloseMove']],prefix='char_dev', seed = 68, ret=1)\n",
        "dev_prod_sel = {1:\"dev\", 0:\"prod\"}\n",
        "prefix = dev_prod_sel[0]\n",
        "\n",
        "train_data = pd.read_csv(\"ProcessedData/char_{}_train.csv\".format(prefix))\n",
        "test_data = pd.read_csv(\"ProcessedData/char_{}_test.csv\".format(prefix))\n",
        "val_data = pd.read_csv(\"ProcessedData/char_{}_val.csv\".format(prefix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEa2oR7Ml5O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Character to Index Mapping\n",
        "char_to_idx = {v:i+2 for i,v in enumerate(string.printable)}\n",
        "char_to_idx['<unk>'] = 0\n",
        "char_to_idx['<pad>'] = 1\n",
        "\n",
        "# Index to Character Mapping\n",
        "idx_to_char = {char_to_idx[i]:i for i in char_to_idx}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uaunf75l5O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CHAR_VOCAB = len(idx_to_char)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CharNeuralNetDataLoader(tud.Dataset):\n",
        "    \"\"\"\n",
        "    Dataloader returns a sequence of characters convrted to indices. \n",
        "    \"\"\"\n",
        "    def __init__(self, data, char_to_idx, idx_to_char, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.idx_to_char = idx_to_char\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of Examples\n",
        "        '''\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple of text and label at the given index.\n",
        "        If label is not present None is returned.\n",
        "        \"\"\"\n",
        "        MAX_LEN = 5600\n",
        "        itm = []\n",
        "        l = 0\n",
        "        for char in self.data.iloc[idx,0].strip():\n",
        "            indx = self.char_to_idx.get(char,0)\n",
        "            itm.append(indx)\n",
        "            l += 1\n",
        "            if l == MAX_LEN:\n",
        "                break\n",
        "        \n",
        "        if len(itm) < MAX_LEN:\n",
        "            itm  = itm + [1 for i in range(MAX_LEN-len(itm))]\n",
        "        \n",
        "        itm = torch.tensor(itm).long()\n",
        "        # To Differentiate Train and Test data\n",
        "        if self.data.shape[1] == 2:\n",
        "            label = self.data.iloc[idx,1]\n",
        "            return itm, label\n",
        "        else:\n",
        "            return itm, None\n",
        "\n",
        "train_dataset = CharNeuralNetDataLoader(train_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
        "val_dataset = CharNeuralNetDataLoader(val_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)\n",
        "test_dataset = CharNeuralNetDataLoader(test_data, char_to_idx, idx_to_char, MAX_CHAR_VOCAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6YQG9jIl5PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_iter = tud.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "test_iter = tud.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
        "val_iter = tud.DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg4OZHAbl5PC",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqdgD-1yl5PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNNClassifier(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Character Level RNN Model\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, embd_dim, pad_index):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.nhid = hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings = input_dim, embedding_dim=embd_dim, padding_idx=pad_index)   \n",
        "        self.rnn = nn.GRU(embd_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout()\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embds = self.embedding(text).float()\n",
        "        embds = embds.permute(1,0,2)\n",
        "        hidden = torch.zeros((1,embds.size(1), self.nhid))\n",
        "        if torch.cuda.is_available():\n",
        "            hidden = hidden.cuda() \n",
        "        out, hid = self.rnn(embds, hidden)       \n",
        "        hid = self.dropout(hid)\n",
        "        out = self.output(hid.squeeze(0))\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNWeCaTl5PF",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DedNftTJl5PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(char_to_idx)\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = 1\n",
        "HIDDEN_DIM = 64\n",
        "EMBD_DIM = 128\n",
        "crnn_model = CharRNNClassifier(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, EMBD_DIM, PAD_IDX)\n",
        "if torch.cuda.is_available():\n",
        "    crnn_model = crnn_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHXnhhKJl5PH",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmDA2BBrl5PI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "340e6da4-592b-405c-f7e8-e7d680d4fd23"
      },
      "source": [
        "CharRNNTrainer = TrainingModule(crnn_model)\n",
        "best_charnn_model = CharRNNTrainer.train_model(train_iter, val_iter)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:    0 | Loss : 0.7096 | Accuracy : 0.4688\n",
            "Iteration:   30 | Loss : 0.6955 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6701 | Accuracy : 0.6250\n",
            "Iteration:   90 | Loss : 0.6686 | Accuracy : 0.6250\n",
            "Iteration:  120 | Loss : 0.6653 | Accuracy : 0.6250\n",
            "Iteration:  150 | Loss : 0.7099 | Accuracy : 0.5156\n",
            "Iteration:  180 | Loss : 0.7084 | Accuracy : 0.5156\n",
            "Iteration:  210 | Loss : 0.6602 | Accuracy : 0.6250\n",
            "Iteration:  240 | Loss : 0.6798 | Accuracy : 0.6250\n",
            "Iteration:  270 | Loss : 0.7049 | Accuracy : 0.5156\n",
            "Iteration:  300 | Loss : 0.6841 | Accuracy : 0.5625\n",
            "Iteration:  330 | Loss : 0.6932 | Accuracy : 0.5781\n",
            "Iteration:  360 | Loss : 0.6697 | Accuracy : 0.6094\n",
            "Iteration:  390 | Loss : 0.6917 | Accuracy : 0.5781\n",
            "Iteration:  420 | Loss : 0.6871 | Accuracy : 0.5156\n",
            "Iteration:  450 | Loss : 0.6604 | Accuracy : 0.6250\n",
            "Iteration:  480 | Loss : 0.6841 | Accuracy : 0.5781\n",
            "Val. Loss : 0.6769 | Val. Accuracy : 0.5907 | Train. Accuracy: 0.5957\n",
            "Iteration:    0 | Loss : 0.6785 | Accuracy : 0.5938\n",
            "Iteration:   30 | Loss : 0.6748 | Accuracy : 0.6094\n",
            "Iteration:   60 | Loss : 0.6713 | Accuracy : 0.6094\n",
            "Iteration:   90 | Loss : 0.6582 | Accuracy : 0.6094\n",
            "Iteration:  120 | Loss : 0.6628 | Accuracy : 0.6094\n",
            "Iteration:  150 | Loss : 0.7218 | Accuracy : 0.5000\n",
            "Iteration:  180 | Loss : 0.7024 | Accuracy : 0.5469\n",
            "Iteration:  210 | Loss : 0.6429 | Accuracy : 0.6406\n",
            "Iteration:  240 | Loss : 0.6768 | Accuracy : 0.5938\n",
            "Iteration:  270 | Loss : 0.7145 | Accuracy : 0.5156\n",
            "Iteration:  300 | Loss : 0.6754 | Accuracy : 0.5781\n",
            "Iteration:  330 | Loss : 0.6835 | Accuracy : 0.5781\n",
            "Iteration:  360 | Loss : 0.6752 | Accuracy : 0.6094\n",
            "Iteration:  390 | Loss : 0.6765 | Accuracy : 0.5781\n",
            "Iteration:  420 | Loss : 0.6954 | Accuracy : 0.5625\n",
            "Iteration:  450 | Loss : 0.6507 | Accuracy : 0.6719\n",
            "Iteration:  480 | Loss : 0.7015 | Accuracy : 0.5625\n",
            "Val. Loss : 0.6767 | Val. Accuracy : 0.5916 | Train. Accuracy: 0.5956\n",
            "Iteration:    0 | Loss : 0.6792 | Accuracy : 0.5938\n",
            "Iteration:   30 | Loss : 0.6761 | Accuracy : 0.6250\n",
            "Iteration:   60 | Loss : 0.6728 | Accuracy : 0.5938\n",
            "Iteration:   90 | Loss : 0.6801 | Accuracy : 0.6094\n",
            "Iteration:  120 | Loss : 0.6698 | Accuracy : 0.6094\n",
            "Iteration:  150 | Loss : 0.7106 | Accuracy : 0.5156\n",
            "Iteration:  180 | Loss : 0.6926 | Accuracy : 0.5156\n",
            "Iteration:  210 | Loss : 0.6621 | Accuracy : 0.6250\n",
            "Iteration:  240 | Loss : 0.6900 | Accuracy : 0.5312\n",
            "Iteration:  270 | Loss : 0.7115 | Accuracy : 0.5000\n",
            "Iteration:  300 | Loss : 0.6701 | Accuracy : 0.6250\n",
            "Iteration:  330 | Loss : 0.6790 | Accuracy : 0.5781\n",
            "Iteration:  360 | Loss : 0.6684 | Accuracy : 0.6094\n",
            "Iteration:  390 | Loss : 0.6742 | Accuracy : 0.6094\n",
            "Iteration:  420 | Loss : 0.6748 | Accuracy : 0.5469\n",
            "Iteration:  450 | Loss : 0.6484 | Accuracy : 0.6875\n",
            "Iteration:  480 | Loss : 0.6978 | Accuracy : 0.5625\n",
            "Val. Loss : 0.6765 | Val. Accuracy : 0.5908 | Train. Accuracy: 0.5958\n",
            "Iteration:    0 | Loss : 0.6942 | Accuracy : 0.5625\n",
            "Iteration:   30 | Loss : 0.6692 | Accuracy : 0.6094\n",
            "Iteration:   60 | Loss : 0.6672 | Accuracy : 0.5938\n",
            "Iteration:   90 | Loss : 0.6615 | Accuracy : 0.6094\n",
            "Iteration:  120 | Loss : 0.6557 | Accuracy : 0.6094\n",
            "Iteration:  150 | Loss : 0.7142 | Accuracy : 0.5000\n",
            "Iteration:  180 | Loss : 0.7006 | Accuracy : 0.5312\n",
            "Iteration:  210 | Loss : 0.6491 | Accuracy : 0.6406\n",
            "Iteration:  240 | Loss : 0.6846 | Accuracy : 0.5469\n",
            "Iteration:  270 | Loss : 0.7063 | Accuracy : 0.5312\n",
            "Iteration:  300 | Loss : 0.6710 | Accuracy : 0.5938\n",
            "Iteration:  330 | Loss : 0.6742 | Accuracy : 0.5938\n",
            "Iteration:  360 | Loss : 0.6804 | Accuracy : 0.6094\n",
            "Iteration:  390 | Loss : 0.6791 | Accuracy : 0.5625\n",
            "Iteration:  420 | Loss : 0.6893 | Accuracy : 0.5312\n",
            "Iteration:  450 | Loss : 0.6535 | Accuracy : 0.6562\n",
            "Iteration:  480 | Loss : 0.6813 | Accuracy : 0.5625\n",
            "Val. Loss : 0.6766 | Val. Accuracy : 0.5913 | Train. Accuracy: 0.5959\n",
            "Iteration:    0 | Loss : 0.6803 | Accuracy : 0.5938\n",
            "Iteration:   30 | Loss : 0.6766 | Accuracy : 0.5938\n",
            "Iteration:   60 | Loss : 0.6708 | Accuracy : 0.5938\n",
            "Iteration:   90 | Loss : 0.6637 | Accuracy : 0.6094\n",
            "Iteration:  120 | Loss : 0.6657 | Accuracy : 0.6094\n",
            "Iteration:  150 | Loss : 0.7182 | Accuracy : 0.5000\n",
            "Iteration:  180 | Loss : 0.6814 | Accuracy : 0.5625\n",
            "Iteration:  210 | Loss : 0.6483 | Accuracy : 0.6250\n",
            "Iteration:  240 | Loss : 0.6795 | Accuracy : 0.5625\n",
            "Iteration:  270 | Loss : 0.7177 | Accuracy : 0.4844\n",
            "Iteration:  300 | Loss : 0.6754 | Accuracy : 0.6250\n",
            "Iteration:  330 | Loss : 0.6679 | Accuracy : 0.5938\n",
            "Iteration:  360 | Loss : 0.6677 | Accuracy : 0.6094\n",
            "Iteration:  390 | Loss : 0.6729 | Accuracy : 0.6094\n",
            "Iteration:  420 | Loss : 0.6839 | Accuracy : 0.5625\n",
            "Iteration:  450 | Loss : 0.6472 | Accuracy : 0.6719\n",
            "Iteration:  480 | Loss : 0.6880 | Accuracy : 0.5469\n",
            "Val. Loss : 0.6767 | Val. Accuracy : 0.5905 | Train. Accuracy: 0.5975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6nt3Yzsl5PK",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0eDVoml5PL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "338d78ae-e206-45d7-8ddc-5590f86176fa"
      },
      "source": [
        "char_rnn_accuracy = TrainingModule(best_charnn_model).evaluate(test_iter)\n",
        "print(\"Character Level Character Accuracy: {:4.4f}\".format(char_rnn_accuracy[1]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Character Level Character Accuracy: 0.6030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWcKS3syZQXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}